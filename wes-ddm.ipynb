{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9892234,"sourceType":"datasetVersion","datasetId":6075467},{"sourceId":9892775,"sourceType":"datasetVersion","datasetId":6075887},{"sourceId":9892900,"sourceType":"datasetVersion","datasetId":6076001},{"sourceId":9892932,"sourceType":"datasetVersion","datasetId":6076027},{"sourceId":9893052,"sourceType":"datasetVersion","datasetId":6076126}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(\"Jai Shree Ganesha\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-13T08:43:56.258803Z","iopub.execute_input":"2024-11-13T08:43:56.259184Z","iopub.status.idle":"2024-11-13T08:43:56.272653Z","shell.execute_reply.started":"2024-11-13T08:43:56.259137Z","shell.execute_reply":"2024-11-13T08:43:56.271760Z"}},"outputs":[{"name":"stdout","text":"Jai Shree Ganesha\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch \ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T08:56:54.963438Z","iopub.execute_input":"2024-11-13T08:56:54.964425Z","iopub.status.idle":"2024-11-13T08:56:56.713347Z","shell.execute_reply.started":"2024-11-13T08:56:54.964369Z","shell.execute_reply":"2024-11-13T08:56:56.712428Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**IMPORTING LIBRARIES**","metadata":{}},{"cell_type":"code","source":"from abc import abstractmethod\nimport math\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport functools","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T08:57:41.821004Z","iopub.execute_input":"2024-11-13T08:57:41.821620Z","iopub.status.idle":"2024-11-13T08:57:41.827091Z","shell.execute_reply.started":"2024-11-13T08:57:41.821576Z","shell.execute_reply":"2024-11-13T08:57:41.825999Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**CHECKPOINT** \n`For less memory on the cost of computations`","metadata":{}},{"cell_type":"code","source":"\"\"\"\nVarious utilities for neural networks.\n\"\"\"\n\nimport math\n\nimport torch as th\nimport torch.nn as nn\n\n\n# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\nclass SiLU(nn.Module):\n    def forward(self, x):\n        return x * th.sigmoid(x)\n\n\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return nn.Linear(*args, **kwargs)\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef update_ema(target_params, source_params, rate=0.99):\n    \"\"\"\n    Update target parameters to be closer to those of source parameters using\n    an exponential moving average.\n\n    :param target_params: the target parameter sequence.\n    :param source_params: the source parameter sequence.\n    :param rate: the EMA rate (closer to 1 means slower).\n    \"\"\"\n    for targ, src in zip(target_params, source_params):\n        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().mul_(scale)\n    return module\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels)\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = th.exp(\n        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n    ).to(device=timesteps.device)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)\n\n\nclass CheckpointFunction(th.autograd.Function):\n    @staticmethod\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n        with th.no_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        return output_tensors\n\n    @staticmethod\n    def backward(ctx, *output_grads):\n        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n        with th.enable_grad():\n            # Fixes a bug where the first op in run_function modifies the\n            # Tensor storage in place, which is not allowed for detach()'d\n            # Tensors.\n            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n            output_tensors = ctx.run_function(*shallow_copies)\n        input_grads = th.autograd.grad(\n            output_tensors,\n            ctx.input_tensors + ctx.input_params,\n            output_grads,\n            allow_unused=True,\n        )\n        del ctx.input_tensors\n        del ctx.input_params\n        del output_tensors\n        return (None, None) + input_grads","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CONV ND**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n\n    Args:\n        dims (int): The number of dimensions of the convolution.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        nn.Module: The convolution module.\n\n    Raises:\n        ValueError: If `dims` is not one of 1, 2, or 3.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:01:22.880963Z","iopub.execute_input":"2024-11-13T09:01:22.881376Z","iopub.status.idle":"2024-11-13T09:01:22.887708Z","shell.execute_reply.started":"2024-11-13T09:01:22.881338Z","shell.execute_reply":"2024-11-13T09:01:22.886729Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**AVG_POOL_ND**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\ndef avg_pool_nd(\n    dims: int,\n    *args,\n    **kwargs,\n) -> nn.Module:\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n\n    Args:\n        dims: The number of dimensions of the average pooling module.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        nn.Module: The average pooling module.\n\n    Raises:\n        ValueError: If `dims` is not one of 1, 2, or 3.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(\"unsupported dimensions: {}\".format(dims))\n\ndef max_pool_nd(dims: int, *args, **kwargs) -> nn.Module:\n    \"\"\"\n    Create a 1D, 2D, or 3D max pooling module.\n\n    Args:\n        dims: The number of dimensions of the max pooling module.\n        *args: Additional positional arguments.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        nn.Module: The max pooling module.\n\n    Raises:\n        ValueError: If `dims` is not one of 1, 2, or 3.\n    \"\"\"\n    # TODO: Allow specifying the `padding` argument for 3D max pooling\n    if dims == 1:\n        return nn.MaxPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.MaxPool2d(*args, **kwargs)\n    elif dims == 3:\n        if 'padding' in kwargs:\n            padding = kwargs['padding']\n            del kwargs['padding']\n        else:\n            padding = 0\n        return nn.MaxPool3d(*args, padding=padding, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:02:17.412432Z","iopub.execute_input":"2024-11-13T09:02:17.413122Z","iopub.status.idle":"2024-11-13T09:02:17.423759Z","shell.execute_reply.started":"2024-11-13T09:02:17.413081Z","shell.execute_reply":"2024-11-13T09:02:17.422762Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"**LINEAR**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\ndef linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return nn.Linear(*args, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:03:17.621899Z","iopub.execute_input":"2024-11-13T09:03:17.622488Z","iopub.status.idle":"2024-11-13T09:03:17.627159Z","shell.execute_reply.started":"2024-11-13T09:03:17.622447Z","shell.execute_reply":"2024-11-13T09:03:17.626124Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**NORMALIZATION**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n    \ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:03:54.026273Z","iopub.execute_input":"2024-11-13T09:03:54.026952Z","iopub.status.idle":"2024-11-13T09:03:54.032449Z","shell.execute_reply.started":"2024-11-13T09:03:54.026912Z","shell.execute_reply":"2024-11-13T09:03:54.031529Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**TIMESTEP EMBEDDING**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch as th\nimport math\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = th.exp(\n        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n    ).to(device=timesteps.device)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**ZERO MODULE**","metadata":{}},{"cell_type":"code","source":"def zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:04:55.789643Z","iopub.execute_input":"2024-11-13T09:04:55.790006Z","iopub.status.idle":"2024-11-13T09:04:55.794810Z","shell.execute_reply.started":"2024-11-13T09:04:55.789960Z","shell.execute_reply":"2024-11-13T09:04:55.793914Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**CHANGE_I/P---O/P**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\ndef change_input_output_unet(model, in_channels=4, out_channels=8):\n    \"\"\"\n\n    :param model: unet model from guided diffusion code, for 256x256 image input\n    :param in_channels:\n    :param out_channels:\n    :return: the model with the change\n    \"\"\"\n\n    # change the input\n    kernel_size = model.input_blocks[0][0].kernel_size\n    stride = model.input_blocks[0][0].stride\n    padding = model.input_blocks[0][0].padding\n    out_channels_in = model.input_blocks[0][0].out_channels\n    model.input_blocks[0][0] = nn.Conv2d(in_channels, out_channels_in, kernel_size, stride, padding)\n\n    # change the input\n    kernel_size = model.out[-1].kernel_size\n    stride = model.out[-1].stride\n    padding = model.out[-1].padding\n    in_channels_out = model.out[-1].in_channels\n    model.out[-1] = nn.Conv2d(in_channels_out, out_channels, kernel_size, stride, padding)\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:05:47.283828Z","iopub.execute_input":"2024-11-13T09:05:47.284431Z","iopub.status.idle":"2024-11-13T09:05:47.292197Z","shell.execute_reply.started":"2024-11-13T09:05:47.284392Z","shell.execute_reply":"2024-11-13T09:05:47.291033Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"**CONVERSION_MODULE**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch as th\nimport torch.nn as nn\n\ndef convert_module_to_f16(l):\n    \"\"\"\n    Convert primitive modules to float16.\n    \"\"\"\n    if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n        l.weight.data = l.weight.data.half()\n        if l.bias is not None:\n            l.bias.data = l.bias.data.half()\n\n\ndef convert_module_to_f32(l):\n    \"\"\"\n    Convert primitive modules to float32, undoing convert_module_to_f16().\n    \"\"\"\n    if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n        l.weight.data = l.weight.data.float()\n        if l.bias is not None:\n            l.bias.data = l.bias.data.float()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:07:24.265677Z","iopub.execute_input":"2024-11-13T09:07:24.266051Z","iopub.status.idle":"2024-11-13T09:07:24.273395Z","shell.execute_reply.started":"2024-11-13T09:07:24.266013Z","shell.execute_reply":"2024-11-13T09:07:24.272381Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"**UNET**","metadata":{}},{"cell_type":"code","source":"NUM_CLASSES = 1000\ndef create_model(\n        image_size,\n        num_channels,\n        num_res_blocks,\n        channel_mult=\"\",\n        learn_sigma=False,\n        class_cond=False,\n        use_checkpoint=False,\n        attention_resolutions=\"16\",\n        num_heads=1,\n        num_head_channels=-1,\n        num_heads_upsample=-1,\n        use_scale_shift_norm=False,\n        dropout=0,\n        resblock_updown=False,\n        use_fp16=False,\n        use_new_attention_order=False,\n        model_path='',\n        pretrain_model='',\n):\n    if channel_mult == \"\":\n        if image_size == 512:\n            channel_mult = (0.5, 1, 1, 2, 2, 4, 4)\n        elif image_size == 256:\n            channel_mult = (1, 1, 2, 2, 4, 4)\n        elif image_size == 128:\n            channel_mult = (1, 1, 2, 3, 4)\n        elif image_size == 64:\n            channel_mult = (1, 2, 3, 4)\n        else:\n            raise ValueError(f\"unsupported image size: {image_size}\")\n    else:\n        channel_mult = tuple(int(ch_mult) for ch_mult in channel_mult.split(\",\"))\n\n    attention_ds = []\n    if isinstance(attention_resolutions, int):\n        attention_ds.append(image_size // attention_resolutions)\n    elif isinstance(attention_resolutions, str):\n        for res in attention_resolutions.split(\",\"):\n            attention_ds.append(image_size // int(res))\n    else:\n        raise NotImplementedError\n\n    model = UNetModel(\n        image_size=image_size,\n        in_channels=3,\n        model_channels=num_channels,\n        out_channels=(3 if not learn_sigma else 6),\n        num_res_blocks=num_res_blocks,\n        attention_resolutions=tuple(attention_ds),\n        dropout=dropout,\n        channel_mult=channel_mult,\n        num_classes=(NUM_CLASSES if class_cond else None),\n        use_checkpoint=use_checkpoint,\n        use_fp16=use_fp16,\n        num_heads=num_heads,\n        num_head_channels=num_head_channels,\n        num_heads_upsample=num_heads_upsample,\n        use_scale_shift_norm=use_scale_shift_norm,\n        resblock_updown=resblock_updown,\n        use_new_attention_order=use_new_attention_order,\n    )\n\n    # update number of channels according the pretrained model\n    if pretrain_model == \"osmosis\":\n        model = change_input_output_unet(model, in_channels=4, out_channels=8)\n\n    try:\n        model.load_state_dict(th.load(model_path, map_location='cpu'))\n    except Exception as e:\n        print(f\"Got exception: {e} / Randomly initialize\")\n    return model\n\n\nclass AttentionPool2d(nn.Module):\n    \"\"\"\n    Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\n    \"\"\"\n\n    def __init__(\n            self,\n            spacial_dim: int,\n            embed_dim: int,\n            num_heads_channels: int,\n            output_dim: int = None,\n    ):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(\n            th.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5\n        )\n        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n        self.num_heads = embed_dim // num_heads_channels\n        self.attention = QKVAttention(self.num_heads)\n\n    def forward(self, x):\n        b, c, *_spatial = x.shape\n        x = x.reshape(b, c, -1)  # NC(HW)\n        x = th.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n        x = self.qkv_proj(x)\n        x = self.attention(x)\n        x = self.c_proj(x)\n        return x[:, :, 0]\n\n\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\n\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    \"\"\"\n    A sequential module that passes timestep embeddings to the children that\n    support it as an extra input.\n    \"\"\"\n\n    def forward(self, x, emb):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb)\n            else:\n                x = layer(x)\n        return x\n\n\nclass Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 upsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        if use_conv:\n            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=1)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        if self.dims == 3:\n            x = F.interpolate(\n                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n            )\n        else:\n            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if self.use_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n                 downsampling occurs in the inner-two dimensions.\n    \"\"\"\n\n    def __init__(self, channels, use_conv, dims=2, out_channels=None):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.dims = dims\n        stride = 2 if dims != 3 else (1, 2, 2)\n        if use_conv:\n            self.op = conv_nd(\n                dims, self.channels, self.out_channels, 3, stride=stride, padding=1\n            )\n        else:\n            assert self.channels == self.out_channels\n            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n\n    def forward(self, x):\n        assert x.shape[1] == self.channels\n        return self.op(x)\n\n\nclass ResBlock(TimestepBlock):\n    \"\"\"\n    A residual block that can optionally change the number of channels.\n\n    :param channels: the number of input channels.\n    :param emb_channels: the number of timestep embedding channels.\n    :param dropout: the rate of dropout.\n    :param out_channels: if specified, the number of out channels.\n    :param use_conv: if True and out_channels is specified, use a spatial\n        convolution instead of a smaller 1x1 convolution to change the\n        channels in the skip connection.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param use_checkpoint: if True, use gradient checkpointing on this module.\n    :param up: if True, use this block for upsampling.\n    :param down: if True, use this block for downsampling.\n    \"\"\"\n\n    def __init__(\n            self,\n            channels,\n            emb_channels,\n            dropout,\n            out_channels=None,\n            use_conv=False,\n            use_scale_shift_norm=False,\n            dims=2,\n            use_checkpoint=False,\n            up=False,\n            down=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(\n                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n            ),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, 3, padding=1\n            )\n        else:\n            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the block to a Tensor, conditioned on a timestep embedding.\n\n        :param x: an [N x C x ...] Tensor of features.\n        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        while len(emb_out.shape) < len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = th.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h\n\n\nclass AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other.\n\n    Originally ported from here, but adapted to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    \"\"\"\n\n    def __init__(\n            self,\n            channels,\n            num_heads=1,\n            num_head_channels=-1,\n            use_checkpoint=False,\n            use_new_attention_order=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        if num_head_channels == -1:\n            self.num_heads = num_heads\n        else:\n            assert (\n                    channels % num_head_channels == 0\n            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n            self.num_heads = channels // num_head_channels\n        self.use_checkpoint = use_checkpoint\n        self.norm = normalization(channels)\n        self.qkv = conv_nd(1, channels, channels * 3, 1)\n        if use_new_attention_order:\n            # split qkv before split heads\n            self.attention = QKVAttention(self.num_heads)\n        else:\n            # split heads before split qkv\n            self.attention = QKVAttentionLegacy(self.num_heads)\n\n        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n\n    def forward(self, x):\n        return checkpoint(self._forward, (x,), self.parameters(), True)\n\n    def _forward(self, x):\n        b, c, *spatial = x.shape\n        x = x.reshape(b, c, -1)\n        qkv = self.qkv(self.norm(x))\n        h = self.attention(qkv)\n        h = self.proj_out(h)\n        return (x + h).reshape(b, c, *spatial)\n\n\ndef count_flops_attn(model, _x, y):\n    \"\"\"\n    A counter for the `thop` package to count the operations in an\n    attention operation.\n    Meant to be used like:\n        macs, params = thop.profile(\n            model,\n            inputs=(inputs, timestamps),\n            custom_ops={QKVAttention: QKVAttention.count_flops},\n        )\n    \"\"\"\n    b, c, *spatial = y[0].shape\n    num_spatial = int(np.prod(spatial))\n    # We perform two matmuls with the same number of ops.\n    # The first computes the weight matrix, the second computes\n    # the combination of the value vectors.\n    matmul_ops = 2 * b * (num_spatial ** 2) * c\n    model.total_ops += th.DoubleTensor([matmul_ops])\n\n\nclass QKVAttentionLegacy(nn.Module):\n    \"\"\"\n    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n\n        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs->bts\", q * scale, k * scale\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs->bct\", weight, v)\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass QKVAttention(nn.Module):\n    \"\"\"\n    A module which performs QKV attention and splits in a different order.\n    \"\"\"\n\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n\n    def forward(self, qkv):\n        \"\"\"\n        Apply QKV attention.\n\n        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n        :return: an [N x (H * C) x T] tensor after attention.\n        \"\"\"\n        bs, width, length = qkv.shape\n        assert width % (3 * self.n_heads) == 0\n        ch = width // (3 * self.n_heads)\n        q, k, v = qkv.chunk(3, dim=1)\n        scale = 1 / math.sqrt(math.sqrt(ch))\n        weight = th.einsum(\n            \"bct,bcs->bts\",\n            (q * scale).view(bs * self.n_heads, ch, length),\n            (k * scale).view(bs * self.n_heads, ch, length),\n        )  # More stable with f16 than dividing afterwards\n        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n        a = th.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n        return a.reshape(bs, -1, length)\n\n    @staticmethod\n    def count_flops(model, _x, y):\n        return count_flops_attn(model, _x, y)\n\n\nclass UNetModel(nn.Module):\n    \"\"\"\n    The full UNet model with attention and timestep embedding.\n\n    :param in_channels: channels in the input Tensor.\n    :param model_channels: base channel count for the model.\n    :param out_channels: channels in the output Tensor.\n    :param num_res_blocks: number of residual blocks per downsample.\n    :param attention_resolutions: a collection of downsample rates at which\n        attention will take place. May be a set, list, or tuple.\n        For example, if this contains 4, then at 4x downsampling, attention\n        will be used.\n    :param dropout: the dropout probability.\n    :param channel_mult: channel multiplier for each level of the UNet.\n    :param conv_resample: if True, use learned convolutions for upsampling and\n        downsampling.\n    :param dims: determines if the signal is 1D, 2D, or 3D.\n    :param num_classes: if specified (as an int), then this model will be\n        class-conditional with `num_classes` classes.\n    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n    :param num_heads: the number of attention heads in each attention layer.\n    :param num_heads_channels: if specified, ignore num_heads and instead use\n     a fixed channel width per attention head.\n    :param num_heads_upsample: works with num_heads to set a different number\n                               of heads for upsampling. Deprecated.\n    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n    :param resblock_updown: use residual blocks for up/downsampling.\n    :param use_new_attention_order: use a different attention pattern for potentially\n                                    increased efficiency.\n    \"\"\"\n\n    def __init__(\n            self,\n            image_size,\n            in_channels,\n            model_channels,\n            out_channels,\n            num_res_blocks,\n            attention_resolutions,\n            dropout=0,\n            channel_mult=(1, 2, 4, 8),\n            conv_resample=True,\n            dims=2,\n            num_classes=None,\n            use_checkpoint=False,\n            use_fp16=False,\n            num_heads=1,\n            num_head_channels=-1,\n            num_heads_upsample=-1,\n            use_scale_shift_norm=False,\n            resblock_updown=False,\n            use_new_attention_order=False,\n    ):\n        super().__init__()\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        self.image_size = image_size\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.num_classes = num_classes\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\n        if self.num_classes is not None:\n            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n\n        ch = input_ch = int(channel_mult[0] * model_channels)\n        self.input_blocks = nn.ModuleList(\n            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n        )\n        self._feature_size = ch\n        input_block_chans = [ch]\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for _ in range(num_res_blocks):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(mult * model_channels),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(mult * model_channels)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads,\n                            num_head_channels=num_head_channels,\n                            use_new_attention_order=use_new_attention_order,\n                        )\n                    )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            AttentionBlock(\n                ch,\n                use_checkpoint=use_checkpoint,\n                num_heads=num_heads,\n                num_head_channels=num_head_channels,\n                use_new_attention_order=use_new_attention_order,\n            ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n\n        self.output_blocks = nn.ModuleList([])\n        for level, mult in list(enumerate(channel_mult))[::-1]:\n            for i in range(num_res_blocks + 1):\n                ich = input_block_chans.pop()\n                layers = [\n                    ResBlock(\n                        ch + ich,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(model_channels * mult),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(model_channels * mult)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads_upsample,\n                            num_head_channels=num_head_channels,\n                            use_new_attention_order=use_new_attention_order,\n                        )\n                    )\n                if level and i == num_res_blocks:\n                    out_ch = ch\n                    layers.append(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            up=True,\n                        )\n                        if resblock_updown\n                        else Upsample(ch, conv_resample, dims=dims, out_channels=out_ch)\n                    )\n                    ds //= 2\n                self.output_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n\n        self.out = nn.Sequential(\n            normalization(ch),\n            nn.SiLU(),\n            zero_module(conv_nd(dims, input_ch, out_channels, 3, padding=1)),\n        )\n\n    def convert_to_fp16(self):\n        \"\"\"\n        Convert the torso of the model to float16.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f16)\n        self.middle_block.apply(convert_module_to_f16)\n        self.output_blocks.apply(convert_module_to_f16)\n\n    def convert_to_fp32(self):\n        \"\"\"\n        Convert the torso of the model to float32.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f32)\n        self.middle_block.apply(convert_module_to_f32)\n        self.output_blocks.apply(convert_module_to_f32)\n\n    def forward(self, x, timesteps, y=None):\n        \"\"\"\n        Apply the model to an input batch.\n\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :param y: an [N] Tensor of labels, if class-conditional.\n        :return: an [N x C x ...] Tensor of outputs.\n        \"\"\"\n        assert (y is not None) == (\n                self.num_classes is not None\n        ), \"must specify y if and only if the model is class-conditional\"\n\n        hs = []\n        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n\n        if self.num_classes is not None:\n            assert y.shape == (x.shape[0],)\n            emb = emb + self.label_emb(y)\n\n        h = x.type(self.dtype)\n        for module in self.input_blocks:\n            h = module(h, emb)\n            hs.append(h)\n        h = self.middle_block(h, emb)\n        for module in self.output_blocks:\n            h = th.cat([h, hs.pop()], dim=1)\n            h = module(h, emb)\n        h = h.type(x.dtype)\n        return self.out(h)\n\n\nclass SuperResModel(UNetModel):\n    \"\"\"\n    A UNetModel that performs super-resolution.\n\n    Expects an extra kwarg `low_res` to condition on a low-resolution image.\n    \"\"\"\n\n    def __init__(self, image_size, in_channels, *args, **kwargs):\n        super().__init__(image_size, in_channels * 2, *args, **kwargs)\n\n    def forward(self, x, timesteps, low_res=None, **kwargs):\n        _, _, new_height, new_width = x.shape\n        upsampled = F.interpolate(low_res, (new_height, new_width), mode=\"bilinear\")\n        x = th.cat([x, upsampled], dim=1)\n        return super().forward(x, timesteps, **kwargs)\n\n\nclass EncoderUNetModel(nn.Module):\n    \"\"\"\n    The half UNet model with attention and timestep embedding.\n\n    For usage, see UNet.\n    \"\"\"\n\n    def __init__(\n            self,\n            image_size,\n            in_channels,\n            model_channels,\n            out_channels,\n            num_res_blocks,\n            attention_resolutions,\n            dropout=0,\n            channel_mult=(1, 2, 4, 8),\n            conv_resample=True,\n            dims=2,\n            use_checkpoint=False,\n            use_fp16=False,\n            num_heads=1,\n            num_head_channels=-1,\n            num_heads_upsample=-1,\n            use_scale_shift_norm=False,\n            resblock_updown=False,\n            use_new_attention_order=False,\n            pool=\"adaptive\",\n    ):\n        super().__init__()\n\n        if num_heads_upsample == -1:\n            num_heads_upsample = num_heads\n\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.out_channels = out_channels\n        self.num_res_blocks = num_res_blocks\n        self.attention_resolutions = attention_resolutions\n        self.dropout = dropout\n        self.channel_mult = channel_mult\n        self.conv_resample = conv_resample\n        self.use_checkpoint = use_checkpoint\n        self.dtype = th.float16 if use_fp16 else th.float32\n        self.num_heads = num_heads\n        self.num_head_channels = num_head_channels\n        self.num_heads_upsample = num_heads_upsample\n\n        time_embed_dim = model_channels * 4\n        self.time_embed = nn.Sequential(\n            linear(model_channels, time_embed_dim),\n            nn.SiLU(),\n            linear(time_embed_dim, time_embed_dim),\n        )\n\n        ch = int(channel_mult[0] * model_channels)\n        self.input_blocks = nn.ModuleList(\n            [TimestepEmbedSequential(conv_nd(dims, in_channels, ch, 3, padding=1))]\n        )\n        self._feature_size = ch\n        input_block_chans = [ch]\n        ds = 1\n        for level, mult in enumerate(channel_mult):\n            for _ in range(num_res_blocks):\n                layers = [\n                    ResBlock(\n                        ch,\n                        time_embed_dim,\n                        dropout,\n                        out_channels=int(mult * model_channels),\n                        dims=dims,\n                        use_checkpoint=use_checkpoint,\n                        use_scale_shift_norm=use_scale_shift_norm,\n                    )\n                ]\n                ch = int(mult * model_channels)\n                if ds in attention_resolutions:\n                    layers.append(\n                        AttentionBlock(\n                            ch,\n                            use_checkpoint=use_checkpoint,\n                            num_heads=num_heads,\n                            num_head_channels=num_head_channels,\n                            use_new_attention_order=use_new_attention_order,\n                        )\n                    )\n                self.input_blocks.append(TimestepEmbedSequential(*layers))\n                self._feature_size += ch\n                input_block_chans.append(ch)\n            if level != len(channel_mult) - 1:\n                out_ch = ch\n                self.input_blocks.append(\n                    TimestepEmbedSequential(\n                        ResBlock(\n                            ch,\n                            time_embed_dim,\n                            dropout,\n                            out_channels=out_ch,\n                            dims=dims,\n                            use_checkpoint=use_checkpoint,\n                            use_scale_shift_norm=use_scale_shift_norm,\n                            down=True,\n                        )\n                        if resblock_updown\n                        else Downsample(\n                            ch, conv_resample, dims=dims, out_channels=out_ch\n                        )\n                    )\n                )\n                ch = out_ch\n                input_block_chans.append(ch)\n                ds *= 2\n                self._feature_size += ch\n\n        self.middle_block = TimestepEmbedSequential(\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n            AttentionBlock(\n                ch,\n                use_checkpoint=use_checkpoint,\n                num_heads=num_heads,\n                num_head_channels=num_head_channels,\n                use_new_attention_order=use_new_attention_order,\n            ),\n            ResBlock(\n                ch,\n                time_embed_dim,\n                dropout,\n                dims=dims,\n                use_checkpoint=use_checkpoint,\n                use_scale_shift_norm=use_scale_shift_norm,\n            ),\n        )\n        self._feature_size += ch\n        self.pool = pool\n        if pool == \"adaptive\":\n            self.out = nn.Sequential(\n                normalization(ch),\n                nn.SiLU(),\n                nn.AdaptiveAvgPool2d((1, 1)),\n                zero_module(conv_nd(dims, ch, out_channels, 1)),\n                nn.Flatten(),\n            )\n        elif pool == \"attention\":\n            assert num_head_channels != -1\n            self.out = nn.Sequential(\n                normalization(ch),\n                nn.SiLU(),\n                AttentionPool2d(\n                    (image_size // ds), ch, num_head_channels, out_channels\n                ),\n            )\n        elif pool == \"spatial\":\n            self.out = nn.Sequential(\n                nn.Linear(self._feature_size, 2048),\n                nn.ReLU(),\n                nn.Linear(2048, self.out_channels),\n            )\n        elif pool == \"spatial_v2\":\n            self.out = nn.Sequential(\n                nn.Linear(self._feature_size, 2048),\n                normalization(2048),\n                nn.SiLU(),\n                nn.Linear(2048, self.out_channels),\n            )\n        else:\n            raise NotImplementedError(f\"Unexpected {pool} pooling\")\n\n    def convert_to_fp16(self):\n        \"\"\"\n        Convert the torso of the model to float16.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f16)\n        self.middle_block.apply(convert_module_to_f16)\n\n    def convert_to_fp32(self):\n        \"\"\"\n        Convert the torso of the model to float32.\n        \"\"\"\n        self.input_blocks.apply(convert_module_to_f32)\n        self.middle_block.apply(convert_module_to_f32)\n\n    def forward(self, x, timesteps):\n        \"\"\"\n        Apply the model to an input batch.\n\n        :param x: an [N x C x ...] Tensor of inputs.\n        :param timesteps: a 1-D batch of timesteps.\n        :return: an [N x K] Tensor of outputs.\n        \"\"\"\n        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n\n        results = []\n        h = x.type(self.dtype)\n        for module in self.input_blocks:\n            h = module(h, emb)\n            if self.pool.startswith(\"spatial\"):\n                results.append(h.type(x.dtype).mean(dim=(2, 3)))\n        h = self.middle_block(h, emb)\n        if self.pool.startswith(\"spatial\"):\n            results.append(h.type(x.dtype).mean(dim=(2, 3)))\n            h = th.cat(results, axis=-1)\n            return self.out(h)\n        else:\n            h = h.type(x.dtype)\n            return self.out(h)\n\n\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=2, padding=padw)] + [nn.Dropout(0.5)]\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\nclass GANLoss(nn.Module):\n    \"\"\"Define different GAN objectives.\n\n    The GANLoss class abstracts away the need to create the target label tensor\n    that has the same size as the input.\n    \"\"\"\n\n    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n        \"\"\" Initialize the GANLoss class.\n\n        Parameters:\n            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n            target_real_label (bool) - - label for a real image\n            target_fake_label (bool) - - label of a fake image\n\n        Note: Do not use sigmoid as the last layer of Discriminator.\n        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n        \"\"\"\n        super(GANLoss, self).__init__()\n        self.register_buffer('real_label', th.tensor(target_real_label))\n        self.register_buffer('fake_label', th.tensor(target_fake_label))\n        self.gan_mode = gan_mode\n        if gan_mode == 'lsgan':\n            self.loss = nn.MSELoss()\n        elif gan_mode == 'vanilla':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif gan_mode in ['wgangp']:\n            self.loss = None\n        else:\n            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n\n    def get_target_tensor(self, prediction, target_is_real):\n        \"\"\"Create label tensors with the same size as the input.\n\n        Parameters:\n            prediction (tensor) - - tpyically the prediction from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n\n        Returns:\n            A label tensor filled with ground truth label, and with the size of the input\n        \"\"\"\n\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(prediction)\n\n    def __call__(self, prediction, target_is_real):\n        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n\n        Parameters:\n            prediction (tensor) - - tpyically the prediction output from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n\n        Returns:\n            the calculated loss.\n        \"\"\"\n        if self.gan_mode in ['lsgan', 'vanilla']:\n            target_tensor = self.get_target_tensor(prediction, target_is_real)\n            loss = self.loss(prediction, target_tensor)\n        elif self.gan_mode == 'wgangp':\n            if target_is_real:\n                loss = -prediction.mean()\n            else:\n                loss = prediction.mean()\n        return loss\n\n\ndef cal_gradient_penalty(netD, real_data, fake_data, device, type='mixed', constant=1.0, lambda_gp=10.0):\n    \"\"\"Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n\n    Arguments:\n        netD (network)              -- discriminator network\n        real_data (tensor array)    -- real images\n        fake_data (tensor array)    -- generated images from the generator\n        device (str)                -- GPU / CPU: from torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n        lambda_gp (float)           -- weight for this loss\n\n    Returns the gradient penalty loss\n    \"\"\"\n    if lambda_gp > 0.0:\n        if type == 'real':  # either use real images, fake images, or a linear interpolation of two.\n            interpolatesv = real_data\n        elif type == 'fake':\n            interpolatesv = fake_data\n        elif type == 'mixed':\n            alpha = th.rand(real_data.shape[0], 1, device=device)\n            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(\n                *real_data.shape)\n            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n        else:\n            raise NotImplementedError('{} not implemented'.format(type))\n        interpolatesv.requires_grad_(True)\n        disc_interpolates = netD(interpolatesv)\n        gradients = th.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n                                     grad_outputs=th.ones(disc_interpolates.size()).to(device),\n                                     create_graph=True, retain_graph=True, only_inputs=True)\n        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp  # added eps\n        return gradient_penalty, gradients\n    else:\n        return 0.0, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:09:26.920809Z","iopub.execute_input":"2024-11-13T09:09:26.921552Z","iopub.status.idle":"2024-11-13T09:09:27.054170Z","shell.execute_reply.started":"2024-11-13T09:09:26.921512Z","shell.execute_reply":"2024-11-13T09:09:27.053255Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"**DYNAMIC THRESHOLDING**","metadata":{}},{"cell_type":"code","source":"import torch \n\"\"\"\nHelper functions for new types of inverse problems\n\"\"\"\ndef normalize(img, s=0.95):\n    scaling = torch.quantile(img.abs(), s)\n    return img * scaling\n\ndef dynamic_thresholding(img, s=0.95):\n    img = normalize(img, s=s)\n    return torch.clip(img, -1., 1.)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:11:12.853612Z","iopub.execute_input":"2024-11-13T09:11:12.853954Z","iopub.status.idle":"2024-11-13T09:11:12.859609Z","shell.execute_reply.started":"2024-11-13T09:11:12.853922Z","shell.execute_reply":"2024-11-13T09:11:12.858663Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"**POSTERIOR MEAN VARIANCE**","metadata":{}},{"cell_type":"code","source":"from abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch\n\n\n# ====================\n# Model Mean Processor\n# ====================\n\n__MODEL_MEAN_PROCESSOR__ = {}\n\n\ndef register_mean_processor(name: str):\n    def wrapper(cls):\n        if __MODEL_MEAN_PROCESSOR__.get(name, None):\n            raise NameError(f\"Name {name} is already registerd.\")\n        __MODEL_MEAN_PROCESSOR__[name] = cls\n        return cls\n\n    return wrapper\n\n\ndef get_mean_processor(name: str, **kwargs):\n    if __MODEL_MEAN_PROCESSOR__.get(name, None) is None:\n        raise NameError(f\"Name {name} is not defined.\")\n    return __MODEL_MEAN_PROCESSOR__[name](**kwargs)\n\n\nclass MeanProcessor(ABC):\n    \"\"\"Predict x_start and calculate mean value\"\"\"\n\n    @abstractmethod\n    def __init__(self, betas, dynamic_threshold, clip_denoised):\n        self.dynamic_threshold = dynamic_threshold\n        self.clip_denoised = clip_denoised\n\n    @abstractmethod\n    def get_mean_and_xstart(self, x, t, model_output):\n        pass\n\n    def process_xstart(self, x):\n        if self.dynamic_threshold:\n            x = dynamic_thresholding(x, s=0.98)\n\n        if self.clip_denoised:\n            x = x.clamp(-1, 1)\n\n        return x\n\n\n@register_mean_processor(name='previous_x')\nclass PreviousXMeanProcessor(MeanProcessor):\n    def __init__(self, betas, dynamic_threshold, clip_denoised):\n        super().__init__(betas, dynamic_threshold, clip_denoised)\n        alphas = 1.0 - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n\n        self.posterior_mean_coef1 = betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        self.posterior_mean_coef2 = (1.0 - alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - alphas_cumprod)\n\n    def predict_xstart(self, x_t, t, x_prev):\n        coef1 = extract_and_expand(1.0 / self.posterior_mean_coef1, t, x_t)\n        coef2 = extract_and_expand(self.posterior_mean_coef2 / self.posterior_mean_coef1, t, x_t)\n        return coef1 * x_prev - coef2 * x_t\n\n    def get_mean_and_xstart(self, x, t, model_output):\n        mean = model_output\n        pred_xstart = self.process_xstart(self.predict_xstart(x, t, model_output))\n        return mean, pred_xstart\n\n\n@register_mean_processor(name='start_x')\nclass StartXMeanProcessor(MeanProcessor):\n    def __init__(self, betas, dynamic_threshold, clip_denoised):\n        super().__init__(betas, dynamic_threshold, clip_denoised)\n        alphas = 1.0 - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n\n        self.posterior_mean_coef1 = betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        self.posterior_mean_coef2 = (1.0 - alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - alphas_cumprod)\n\n    def q_posterior_mean(self, x_start, x_t, t):\n        \"\"\"\n        Compute the mean of the diffusion posteriro:\n            q(x_{t-1} | x_t, x_0)\n        \"\"\"\n        assert x_start.shape == x_t.shape\n        coef1 = extract_and_expand(self.posterior_mean_coef1, t, x_start)\n        coef2 = extract_and_expand(self.posterior_mean_coef2, t, x_t)\n\n        return coef1 * x_start + coef2 * x_t\n\n    def get_mean_and_xstart(self, x, t, model_output):\n        pred_xstart = self.process_xstart(model_output)\n        mean = self.q_posterior_mean(x_start=pred_xstart, x_t=x, t=t)\n\n        return mean, pred_xstart\n\n\n@register_mean_processor(name='epsilon')\nclass EpsilonXMeanProcessor(MeanProcessor):\n    def __init__(self, betas, dynamic_threshold, clip_denoised):\n        super().__init__(betas, dynamic_threshold, clip_denoised)\n        alphas = 1.0 - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n\n        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / alphas_cumprod)\n        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / alphas_cumprod - 1)\n        self.posterior_mean_coef1 = betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        self.posterior_mean_coef2 = (1.0 - alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - alphas_cumprod)\n\n    def q_posterior_mean(self, x_start, x_t, t):\n        \"\"\"\n        Compute the mean of the diffusion posteriro:\n            q(x_{t-1} | x_t, x_0)\n        \"\"\"\n        assert x_start.shape == x_t.shape\n        coef1 = extract_and_expand(self.posterior_mean_coef1, t, x_start)\n        coef2 = extract_and_expand(self.posterior_mean_coef2, t, x_t)\n        return coef1 * x_start + coef2 * x_t\n\n    def predict_xstart(self, x_t, t, eps):\n        coef1 = extract_and_expand(self.sqrt_recip_alphas_cumprod, t, x_t)\n        coef2 = extract_and_expand(self.sqrt_recipm1_alphas_cumprod, t, eps)\n        return coef1 * x_t - coef2 * eps\n\n    def get_mean_and_xstart(self, x, t, model_output):\n        pred_xstart = self.process_xstart(self.predict_xstart(x, t, model_output))\n        mean = self.q_posterior_mean(pred_xstart, x, t)\n\n        return mean, pred_xstart\n\n\n# =========================\n# Model Variance Processor\n# =========================\n\n__MODEL_VAR_PROCESSOR__ = {}\n\n\ndef register_var_processor(name: str):\n    def wrapper(cls):\n        if __MODEL_VAR_PROCESSOR__.get(name, None):\n            raise NameError(f\"Name {name} is already registerd.\")\n        __MODEL_VAR_PROCESSOR__[name] = cls\n        return cls\n\n    return wrapper\n\n\ndef get_var_processor(name: str, **kwargs):\n    if __MODEL_VAR_PROCESSOR__.get(name, None) is None:\n        raise NameError(f\"Name {name} is not defined.\")\n    return __MODEL_VAR_PROCESSOR__[name](**kwargs)\n\n\nclass VarianceProcessor(ABC):\n    @abstractmethod\n    def __init__(self, betas):\n        pass\n\n    @abstractmethod\n    def get_variance(self, x, t):\n        pass\n\n\n@register_var_processor(name='fixed_small')\nclass FixedSmallVarianceProcessor(VarianceProcessor):\n    def __init__(self, betas):\n        alphas = 1.0 - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        self.posterior_variance = (\n                betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        )\n\n    def get_variance(self, x, t):\n        model_variance = self.posterior_variance\n        model_log_variance = np.log(model_variance)\n\n        model_variance = extract_and_expand(model_variance, t, x)\n        model_log_variance = extract_and_expand(model_log_variance, t, x)\n\n        return model_variance, model_log_variance\n\n\n@register_var_processor(name='fixed_large')\nclass FixedLargeVarianceProcessor(VarianceProcessor):\n    def __init__(self, betas):\n        self.betas = betas\n\n        alphas = 1.0 - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        self.posterior_variance = (\n                betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        )\n\n    def get_variance(self, x, t):\n        model_variance = np.append(self.posterior_variance[1], self.betas[1:])\n        model_log_variance = np.log(model_variance)\n\n        model_variance = extract_and_expand(model_variance, t, x)\n        model_log_variance = extract_and_expand(model_log_variance, t, x)\n\n        return model_variance, model_log_variance\n\n\n@register_var_processor(name='learned')\nclass LearnedVarianceProcessor(VarianceProcessor):\n    def __init__(self, betas):\n        pass\n\n    def get_variance(self, x, t):\n        model_log_variance = x\n        model_variance = torch.exp(model_log_variance)\n        return model_variance, model_log_variance\n\n\n@register_var_processor(name='learned_range')\nclass LearnedRangeVarianceProcessor(VarianceProcessor):\n    def __init__(self, betas):\n        self.betas = betas\n\n        alphas = 1.0 - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (\n                betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n        )\n        # log calculation clipped because the posterior variance is 0 at the\n        # beginning of the diffusion chain.\n        self.posterior_log_variance_clipped = np.log(\n            np.append(posterior_variance[1], posterior_variance[1:])\n        )\n\n    def get_variance(self, x, t):\n        model_var_values = x\n        min_log = self.posterior_log_variance_clipped\n        max_log = np.log(self.betas)\n\n        min_log = extract_and_expand(min_log, t, x)\n        max_log = extract_and_expand(max_log, t, x)\n\n        # The model_var_values is [-1, 1] for [min_var, max_var]\n        frac = (model_var_values + 1.0) / 2.0\n        model_log_variance = frac * max_log + (1 - frac) * min_log\n        model_variance = torch.exp(model_log_variance)\n        return model_variance, model_log_variance\n\n\n# ================\n# Helper function\n# ================\n\ndef extract_and_expand(array, time, target):\n    array = torch.from_numpy(array).to(target.device)[time].float()\n    while array.ndim < target.ndim:\n        array = array.unsqueeze(-1)\n    return array.expand_as(target)\n\n\ndef expand_as(array, target):\n    if isinstance(array, np.ndarray):\n        array = torch.from_numpy(array)\n    elif isinstance(array, np.float):\n        array = torch.tensor([array])\n\n    while array.ndim < target.ndim:\n        array = array.unsqueeze(-1)\n\n    return array.expand_as(target).to(target.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:12:14.923715Z","iopub.execute_input":"2024-11-13T09:12:14.924048Z","iopub.status.idle":"2024-11-13T09:12:14.966186Z","shell.execute_reply.started":"2024-11-13T09:12:14.924015Z","shell.execute_reply":"2024-11-13T09:12:14.965269Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**utils**","metadata":{}},{"cell_type":"code","source":"\nimport sys\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nimport yaml\nimport argparse\nimport datetime\nimport re\nfrom PIL import Image, ImageDraw, ImageFont\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.loss import _Loss\nimport torch.optim as optim\nimport torchvision.transforms.functional as tvtf\n\n\n# %% image functions\n\ndef min_max_norm(img, global_norm=True, is_uint8=True):\n    \"\"\"\n    assume input is a torch tensor [3,h,w]\n    \"\"\"\n    if global_norm:\n        img_norm = img - img.min()\n        img_norm /= img_norm.max()\n    else:\n        img_norm = torch.zeros_like(img)\n\n        img_norm[0, :, :] = img[0, :, :] - img[0, :, :].min()\n        img_norm[1, :, :] = img[1, :, :] - img[1, :, :].min()\n        img_norm[2, :, :] = img[2, :, :] - img[2, :, :].min()\n\n        img_norm[0, :, :] = img_norm[0, :, :] / img_norm[0, :, :].max()\n        img_norm[1, :, :] = img_norm[1, :, :] / img_norm[1, :, :].max()\n        img_norm[2, :, :] = img_norm[2, :, :] / img_norm[2, :, :].max()\n\n    if is_uint8:\n        img_norm *= 255\n        img_norm = img_norm.to(torch.uint8)\n\n    return img_norm\n\n\ndef min_max_norm_range(img, vmin=0, vmax=1, is_uint8=False):\n    \"\"\"\n    assume input is a torch tensor [3/1,h,w] or [Batch,3/1,h,w]\n    \"\"\"\n\n    vmin = float(vmin)\n    vmax = float(vmax)\n\n    # Compute the minimum and maximum values for each image in the batch separately\n    if len(img.shape) == 4:\n        # support a batch\n        img_min = img.view(img.size(0), -1).min(dim=1, keepdim=True)[0].view(-1, 1, 1, 1)\n        img_max = img.view(img.size(0), -1).max(dim=1, keepdim=True)[0].view(-1, 1, 1, 1)\n\n    elif len(img.shape) == 3:\n        img_max = img.max()\n        img_min = img.min()\n\n    else:\n        raise NotImplementedError\n\n    if img_min == img_max:\n        img_norm = torch.zeros_like(img)\n    else:\n        scale = (vmax - vmin) / (img_max - img_min)\n        img_norm = (img - img_min) * scale + vmin\n\n    if is_uint8:\n        img_norm = (255 * img_norm).to(torch.uint8)\n\n    return img_norm\n\n\ndef min_max_norm_range_percentile(img, vmin=0, vmax=1, percent_low=0., percent_high=1., is_uint8=False):\n    \"\"\"\n    assume input is a torch tensor [3/1,h,w]\n    \"\"\"\n\n    # first clip into percentile values\n    img_min = torch.quantile(img, q=percent_low)\n    img_max = torch.quantile(img, q=percent_high)\n    img_clip = torch.clamp(img, img_min, img_max)\n\n    vmin = float(vmin)\n    vmax = float(vmax)\n\n    # Compute the minimum and maximum values for each image in the batch separately\n    if len(img_clip.shape) == 4:\n        # support a batch\n        img_min = img_clip.view(img.size(0), -1).min(dim=1, keepdim=True)[0].view(-1, 1, 1, 1)\n        img_max = img_clip.view(img.size(0), -1).max(dim=1, keepdim=True)[0].view(-1, 1, 1, 1)\n\n    elif len(img.shape) == 3:\n        img_max = img_clip.max()\n        img_min = img_clip.min()\n\n    else:\n        raise NotImplementedError\n\n    if img_min == img_max:\n        img_norm = torch.zeros_like(img_clip)\n    else:\n        scale = (vmax - vmin) / (img_max - img_min)\n        img_norm = (img_clip - img_min) * scale + vmin\n\n    if is_uint8:\n        img_norm = (255 * img_norm).to(torch.uint8)\n\n    return img_norm\n\n\ndef max_norm(img, global_norm=True, is_uint8=True):\n    \"\"\"\n    assume input is a torch tensor [3,h,w]\n    \"\"\"\n\n    if global_norm:\n        img_norm = img / img.max()\n\n    else:\n        img_norm = torch.zeros_like(img)\n        img_norm[0, :, :] = img[0, :, :] / img[0, :, :].max()\n        img_norm[1, :, :] = img[1, :, :] / img[1, :, :].max()\n        img_norm[2, :, :] = img[2, :, :] / img[2, :, :].max()\n\n    if is_uint8:\n        img_norm *= 255\n        img_norm = img_norm.to(torch.uint8)\n\n    return img_norm\n\n\ndef clip_image(img, scale=True, move=True, is_uint8=True):\n    \"\"\"\n    assume input is a torch tensor [ch,h,w]\n    ch can be 3/1\n    \"\"\"\n\n    # fix in case the image is only [imagesize, imagesize]\n    if len(img.shape) == 2:\n        img = img.unsqueeze(0)\n\n    if move:\n        img = img + 1\n    if scale:\n        img = 0.5 * img\n\n    if is_uint8:\n        img *= 255\n        img = img.clamp(0, 255).to(torch.uint8)\n    else:\n        img = img.clamp(0, 1)\n\n    return img\n\n\ndef gaussian_kernel(kernel_size, sigma=1., muu=0.):\n    # Initializing value of x,y as grid of kernel size\n    # in the range of kernel size\n\n    x, y = np.meshgrid(np.linspace(0, kernel_size, kernel_size),\n                       np.linspace(0, kernel_size, kernel_size))\n\n    x -= kernel_size // 2\n    y -= kernel_size // 2\n\n    dst = np.sqrt(x ** 2 + y ** 2)\n\n    # lower normal part of gaussian\n    # normal = 1 / (2 * np.pi * sigma ** 2)\n    normal = 1\n\n    # Calculating Gaussian filter\n    # gauss = normal * np.exp(-((dst - muu) ** 2 / (2.0 * sigma ** 2)))\n    gauss = normal * np.exp(-((dst - muu) ** 2 / (sigma ** 2)))\n\n    return gauss\n\n\ndef create_image_text_to_grid(image, image_size=[256, 256], info_str=\"light factor\", norm=True):\n    \"\"\"\n    input is an image (1 or 3 channels) or a scalar (1 or 3 channels) as pytorch tensor\n    outputs are:\n    1. a tensor image [3, image_size, image_size]\n    2. a text for logger\n    \"\"\"\n    shape = list(image.detach().cpu().shape)\n    image = image.detach().cpu()\n\n    # 1 channel scalar\n    if len(image.detach().cpu().shape) == 1:\n        text = f\"{info_str} = {image.item():.3f}\"\n        out_image = image * torch.ones(size=[3] + image_size)\n        # cast to uint8\n        out_image = (255 * out_image).to(torch.uint8)\n\n    # 3 channels scalar\n    elif len(shape) == 3 and shape[-3] == 3 and (not shape[-2::] == image_size):\n        text = f\"{info_str} = \" \\\n               f\"[{image.squeeze().numpy()[0]:.3f}, \" \\\n               f\"{image.squeeze().numpy()[1]:.3f}, \" \\\n               f\"{image.squeeze().numpy()[2]:.3f}]\"\n        out_image = torch.zeros(size=[3, image_size[0], image_size[1]], dtype=torch.float32)\n        out_image[0] = image[0] * torch.ones(size=image_size)\n        out_image[1] = image[1] * torch.ones(size=image_size)\n        out_image[2] = image[2] * torch.ones(size=image_size)\n        # cast to uint8\n        out_image = (255 * out_image).to(torch.uint8)\n\n    # 1 channel image\n    elif len(shape) == 2 and shape[-2::] == image_size:\n        text = f\"{info_str} mean = {image.mean():.3f}\\n\" \\\n               f\"{info_str} std = {image.std():.3f}\\n\" \\\n               f\"{info_str} min = {image.min():.3f}\\n\" \\\n               f\"{info_str} max = {image.max():.3f}\"\n        out_image = image.unsqueeze(0).repeat(3, 1, 1)\n        out_image = min_max_norm(out_image, is_uint8=True) if norm else (255 * out_image).to(torch.uint8)\n\n    # 3 channels image\n\n    elif len(shape) == 3 and shape[-3] == 3 and shape[-2::] == image_size:\n        text = f\"Red mean={image[0].mean():.3f}, std={image[0].std():.3f}, min={image[0].min():.3f}, max={image[0].max():.3f}\\n\" \\\n               f\"Green mean={image[1].mean():.3f}, std={image[1].std():.3f}, min={image[1].min():.3f}, max={image[1].max():.3f}\\n\" \\\n               f\"Blue mean={image[2].mean():.3f}, std={image[2].std():.3f}, min={image[2].min():.3f}, max={image[2].max():.3f}\\n\"\n        out_image = min_max_norm(image, is_uint8=True) if norm else (255 * image).to(torch.uint8)\n\n    else:\n        ValueError(f\"Image dimensions are not recognized - shape={shape}\")\n\n    return out_image, text\n\n\ndef add_text_torch_img(img, text, font_size=15):\n    \"\"\"\n\n    :param img: torch image shape [3,h,w]\n    :param text: text to insert\n    :return: torch image shape [3,h,w]\n    \"\"\"\n\n    # print betas and b_inf on b_inf image\n    img_pil = tvtf.to_pil_image(img)\n    I_text = ImageDraw.Draw(img_pil)\n\n    if sys.platform.startswith(\"win\"):\n        I_text.font = ImageFont.truetype(\"arial.ttf\", font_size)\n    elif sys.platform.startswith(\"linux\"):\n        I_text.font = ImageFont.truetype(\"/usr/share/fonts/truetype/freefont/FreeMono.ttf\", font_size,\n                                         encoding=\"unic\")\n    else:\n        raise NotImplementedError\n    I_text.multiline_text((5, 30), text, fill=(0, 0, 0))\n    b_inf_image = tvtf.to_tensor(img_pil)\n\n    return b_inf_image\n\n\n# %% change input and outputs of the unet\n\ndef change_input_output_unet(model, in_channels=4, out_channels=8):\n    \"\"\"\n\n    :param model: unet model from guided diffusion code, for 256x256 image input\n    :param in_channels:\n    :param out_channels:\n    :return: the model with the change\n    \"\"\"\n\n    # change the input\n    kernel_size = model.input_blocks[0][0].kernel_size\n    stride = model.input_blocks[0][0].stride\n    padding = model.input_blocks[0][0].padding\n    out_channels_in = model.input_blocks[0][0].out_channels\n    model.input_blocks[0][0] = nn.Conv2d(in_channels, out_channels_in, kernel_size, stride, padding)\n\n    # change the input\n    kernel_size = model.out[-1].kernel_size\n    stride = model.out[-1].stride\n    padding = model.out[-1].padding\n    in_channels_out = model.out[-1].in_channels\n    model.out[-1] = nn.Conv2d(in_channels_out, out_channels, kernel_size, stride, padding)\n\n    return model\n\n\n# %% masked mse loss\n\nclass MaskedMSELoss(_Loss):\n    \"\"\"\n\n    masked version of MSE loss\n\n    \"\"\"\n\n    __constants__ = ['reduction']\n\n    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n        super(MaskedMSELoss, self).__init__(size_average, reduce, reduction)\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        masked_base_loss = mask * F.mse_loss(input, target, reduction='none')\n\n        if self.reduction == 'sum':\n            masked_mse_loss = masked_base_loss.sum()\n        elif self.reduction == 'mean':\n            # the number of channel of mask is 1, and the image the RGB/RGBD therefore a multiplication is required\n            num_channels = input.shape[1]\n            num_non_zero_elements = num_channels * mask.sum()\n            masked_mse_loss = masked_base_loss.sum() / num_non_zero_elements\n        elif self.reduction == 'none':\n            masked_mse_loss = masked_base_loss\n        else:\n            ValueError(f\"Unknown reduction input: {self.reduction}\")\n\n        return masked_mse_loss\n\n\n# %% masked L1 loss\n\nclass MaskedL1Loss(_Loss):\n    \"\"\"\n\n    masked version of L1 loss\n\n    \"\"\"\n\n    __constants__ = ['reduction']\n\n    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n        super(MaskedL1Loss, self).__init__(size_average, reduce, reduction)\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        masked_base_loss = mask * F.l1_loss(input, target, reduction='none')\n\n        if self.reduction == 'sum':\n            masked_l1_loss = masked_base_loss.sum()\n        elif self.reduction == 'mean':\n            # the number of channel of mask is 1, and the image the RGB/RGBD therefore a multiplication is required\n            num_channels = input.shape[1]\n            num_non_zero_elements = num_channels * mask.sum()\n            masked_l1_loss = masked_base_loss.sum() / num_non_zero_elements\n        elif self.reduction == 'none':\n            masked_l1_loss = masked_base_loss\n        else:\n            ValueError(f\"Unknown reduction input: {self.reduction}\")\n\n        return masked_l1_loss\n\n\n# %% read yaml config file and parser functions\n\ndef load_yaml(file_path: str) -> dict:\n    with open(file_path) as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n    return config\n\n\n# read yaml file (config file and write the content into txt file)\n\ndef yaml_to_txt(yaml_file_path, txt_file_path):\n    # Read YAML file\n    with open(yaml_file_path, 'r') as yaml_file:\n        yaml_data = yaml.load(yaml_file, Loader=yaml.FullLoader)\n\n    # Convert YAML data to a string\n    yaml_text = yaml.dump(yaml_data, default_flow_style=False)\n\n    # Write YAML data to a text file\n    with open(txt_file_path, 'w') as txt_file:\n        txt_file.write(yaml_text)\n\n\n# dictionary and argparser functions\n\ndef args_to_dict(args, keys):\n    return {k: getattr(args, k) for k in keys}\n\n\ndef str2bool(v):\n    \"\"\"\n    https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\n    \"\"\"\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"boolean value expected\")\n\n\ndef add_dict_to_argparser(parser, default_dict):\n    \"\"\"\n    function from guided diffusion code\n    \"\"\"\n\n    for k, v in default_dict.items():\n        v_type = type(v)\n        if v is None:\n            v_type = str\n        elif isinstance(v, bool):\n            v_type = str2bool\n        parser.add_argument(f\"--{k}\", default=v, type=v_type)\n\n\ndef add_dict_to_namespace(namespace, args_dict):\n    for key, value in args_dict.items():\n        setattr(namespace, key, value)\n\n\n# save directory using date\ndef update_save_dir_date(arguments_save_dir: str) -> str:\n    today = datetime.date.today()\n    today = f\"{today.day}-{today.month}-{today.year % 2000}\"\n    run_description = \"run1\"\n    save_dir = pjoin(arguments_save_dir, f\"{today}\", run_description)\n\n    # check if this path is already exist\n    while True:\n        if os.path.exists(save_dir):\n\n            digits = re.findall(r'\\d+$', save_dir)[0]\n            digits_len = len(str(digits))\n            save_dir = f\"{save_dir[0:-digits_len]}{int(digits) + 1}\"\n        else:\n            break\n    os.makedirs(save_dir, exist_ok=True)\n\n    return save_dir\n\n\n# checkpoint path update\n\ndef update_checkpoint_path(save_dir_path: str) -> str:\n    checkpoint_path = os.path.join(save_dir_path, \"checkpoint\")\n    os.makedirs(checkpoint_path, exist_ok=True)\n    return os.path.join(checkpoint_path, \"checkpoint.pt\")\n\n\n# update_relevant_arguments function\n\ndef update_relevant_arguments(args, save_dir_path: str):\n    # cast to float relevant inputs\n    args.lr, args.fp16_scale_growth = float(args.lr), float(args.fp16_scale_growth)\n    args.save_dir = update_save_dir_date(args.save_dir_main)\n    args.checkpoint_path = update_checkpoint_path(args.save_dir) if args.save_checkpoint else \"\"\n\n    # specify number of input and output channels according to pretrained model\n    if args.pretrain_model == \"debka\":\n        args.unet_in_channels = 4\n        args.unet_out_channels = (4 if not args.learn_sigma else 8)\n    else:\n        args.unet_in_channels = 3\n        args.unet_out_channels = (3 if not args.learn_sigma else 6)\n\n    return args\n\n\n# arguments parser functions\ndef arguments_from_file(config_file_path: str) -> argparse.Namespace:\n    # read config file\n    args_dict = load_yaml(config_file_path)\n\n    # create argparse Namspace object\n    args = argparse.Namespace()\n\n    # add config dictionary into argparse namespace\n    add_dict_to_namespace(args, args_dict)\n\n    return args\n\n\n# os run\n\ndef get_os():\n    if sys.platform.startswith('linux'):\n        os_run = 'linux'\n    elif sys.platform.startswith('win'):\n        os_run = 'win'\n    else:\n        print(\"Running on a different platform\")\n\n    return os_run\n\n\n# %% return torch optimizer by name\n\ndef get_optimizer(optimizer_name, model_parameters, **kwargs):\n    optimizer_name = optimizer_name.lower()\n\n    if optimizer_name is None or optimizer_name == \"gd\" or optimizer_name == \"\":\n        return None\n    elif optimizer_name == 'adam':\n        return optim.Adam(model_parameters, **kwargs)\n    elif optimizer_name == 'sgd':\n        return optim.SGD(model_parameters, **kwargs)\n    elif optimizer_name == 'rmsprop':\n        return optim.RMSprop(model_parameters, **kwargs)\n    elif optimizer_name == 'adagrad':\n        return optim.Adagrad(model_parameters, **kwargs)\n    elif optimizer_name == 'adadelta':\n        return optim.Adadelta(model_parameters, **kwargs)\n    elif optimizer_name == 'adamw':\n        return optim.AdamW(model_parameters, **kwargs)\n    elif optimizer_name == 'sparseadam':\n        return optim.SparseAdam(model_parameters, **kwargs)\n    elif optimizer_name == 'adamax':\n        return optim.Adamax(model_parameters, **kwargs)\n    elif optimizer_name == 'asgd':\n        return optim.ASGD(model_parameters, **kwargs)\n    elif optimizer_name == 'lbfgs':\n        return optim.LBFGS(model_parameters, **kwargs)\n    elif optimizer_name == 'rprop':\n        return optim.Rprop(model_parameters, **kwargs)\n    elif optimizer_name == 'rprop':\n        return optim.Rprop(model_parameters, **kwargs)\n    else:\n        raise ValueError(f\"Optimizer '{optimizer_name}' is not supported.\")\n\n\n# %% change depth function according to the input of depth type\n\ndef get_depth_value(value_raw, **kwargs):\n    if isinstance(value_raw, float):\n        value = value_raw\n    elif isinstance(value_raw, int):\n        value = float(value_raw)\n    elif isinstance(value_raw, str):\n        value = np.fromstring(value_raw, dtype=float, sep=',')\n    elif isinstance(value_raw, (np.ndarray, np.generic)):\n        value = value_raw\n    else:\n        raise NotImplementedError\n\n    return value\n\n\ndef convert_depth(depth, depth_type, **kwargs):\n    \"\"\"\n\n    :param depth: expected to get the depth as it gets out from the unet model\n    :param depth_type: the type of conversion\n    :return: converted out depth\n    \"\"\"\n    tmp_value = kwargs.get(\"value\", None)\n    value = get_depth_value(tmp_value)\n\n    if depth_type == \"move\":\n        depth_out = depth + value\n\n    elif depth_type == \"gamma\":\n        depth_out = torch.pow((depth + value[0]) * value[1], value[2])\n\n    elif depth_type is None or depth_type == \"original\":\n        depth_out = 0.5 * (depth + 1.0)\n\n    else:\n        raise NotImplementedError\n\n    return depth_out\n\n\n# %% when pattern sampling - check if freezing phi is required\n\ndef is_freeze_phi(sample_pattern, time_index, num_timesteps):\n    # original sampling (no freezing phi required at all)\n    if (sample_pattern is None) or (sample_pattern[\"pattern\"] == \"original\"):\n        freeze_phi = False\n\n        # in case of non guidance for that time index, no alternating happens\n    elif time_index > sample_pattern['start_guidance'] * num_timesteps or \\\n            time_index < sample_pattern['stop_guidance'] * num_timesteps:\n        freeze_phi = True\n\n    # gibbsDDRM pattern sampling - but before starting update phi\n    elif time_index > sample_pattern[\"update_start\"] * num_timesteps or time_index < sample_pattern[\n        \"update_end\"] * num_timesteps:\n        freeze_phi = True\n\n    # otherwise not freezing phi\n    else:\n        freeze_phi = False\n\n    return freeze_phi\n\n\n# %% when pattern sampling - set alternating length\n\ndef set_alternate_length(sample_pattern, time_index, num_timesteps):\n    # check correction of the values\n    if (sample_pattern[\"pattern\"] != \"original\") and (sample_pattern is not None):\n\n        assert sample_pattern[\"update_start\"] > sample_pattern[\"update_end\"]\n        assert sample_pattern[\"s_start\"] > sample_pattern[\"s_end\"]\n\n        if sample_pattern['local_M'] > 1:\n            assert sample_pattern[\"update_start\"] >= sample_pattern[\"s_start\"]\n            assert sample_pattern[\"s_end\"] >= sample_pattern[\"update_end\"]\n\n        # this is the original - non pattern case\n    if (sample_pattern is None) or (sample_pattern[\"pattern\"] == \"original\"):\n        alternate_length = 1\n\n    # in case of non guidance for that time index, no alternating happens\n    elif time_index > sample_pattern['start_guidance'] * num_timesteps or \\\n            time_index < sample_pattern['stop_guidance'] * num_timesteps:\n        alternate_length = 1\n\n    # Until start update there is no optimization of phi's - This is mentioned in the gibbsDDRM paper\n    elif time_index > sample_pattern[\"update_start\"] * num_timesteps or \\\n            time_index < sample_pattern[\"update_end\"] * num_timesteps:\n        alternate_length = 1\n\n    # PGDiff paper - S_start and S_end - time indices which the alternate optimization is happened\n    # in this case S_start should be smaller than update start\n    # s_start should be smaller than update_start and s_end larger than update_end\n    elif time_index > sample_pattern[\"s_start\"] * num_timesteps or \\\n            time_index < sample_pattern[\"s_end\"] * num_timesteps:\n        alternate_length = 1\n\n    else:\n        alternate_length = sample_pattern[\"local_M\"]\n\n    return alternate_length\n\n\n# %% logging text\n\ndef log_text(args):\n    log_txt_tmp = f\"\\n\\nGuidance Scale: {args.conditioning['params']['scale']}\" \\\n                  f\"\\nLoss Function: {args.conditioning['params']['loss_function']}\" \\\n                  f\"\\nweight: {args.conditioning['params']['loss_weight']}, \" \\\n                  f\"weight_function: {args.conditioning['params']['weight_function']}\" \\\n                  f\"\\nAuxiliary Loss: {args.aux_loss['aux_loss']}\" \\\n                  f\"\\nUnderwater model: {args.measurement['operator']['name']}\" \\\n                  f\"\\nOptimize w.r.t: {'x_prev' if args.conditioning['params']['gradient_x_prev'] else 'x0'}\" \\\n                  f\"\\nOptimizer model: {args.measurement['operator']['optimizer'] if 'optimizer' in list(args.measurement['operator'].keys()) else 'none'}, \" \\\n                  f\"\\nManual seed: {args.manual_seed}\" \\\n                  f\"\\nDepth type: {args.measurement['operator']['depth_type']}, value: {args.measurement['operator']['value']}\"\n\n    log_noise_txt = f\"\\nNoise: {args.measurement['noise']['name']}\"\n    if 'sigma' in list(args.measurement['noise'].keys()):\n        log_noise_txt += f\", sigma: {args.measurement['noise']['sigma']}\"\n    log_txt_tmp += log_noise_txt\n\n    gradient_clip_tmp = args.conditioning['params']['gradient_clip']\n    gradient_clip_tmp = [num_str for num_str in gradient_clip_tmp.split(',')]\n    log_grad_clip_txt = f\"\\nGradient Clipping: {gradient_clip_tmp[0]}\"\n    gradient_clip = str2bool(gradient_clip_tmp[0])\n    if gradient_clip:\n        log_grad_clip_txt += f\", min value: -{gradient_clip_tmp[1]}, max value: {gradient_clip_tmp[1]}\"\n    log_txt_tmp += log_grad_clip_txt\n\n    if args.sample_pattern['pattern'] == 'original':\n        log_txt_tmp += f\"\\nSample Pattern: original\"\n    else:\n        log_txt_tmp += f\"\\nSample Pattern: {args.sample_pattern['pattern']}, \" \\\n                       f\"\\n     Guidance start: {args.sample_pattern['start_guidance']} ,end: {args.sample_pattern['stop_guidance']}\" \\\n                       f\"\\n     Optimizations iters: {args.sample_pattern['n_iter']}, \" \\\n                       f\"\\n     Update start from: {args.sample_pattern['update_start']}, end: {args.sample_pattern['update_end']}\" \\\n                       f\"\\n     M: {args.sample_pattern['local_M']}, start: {args.sample_pattern['s_start']}, end: {args.sample_pattern['s_end']}\"\n\n    return log_txt_tmp\n\n\n# %% loss_weight - factor the difference between the  measurement to the degraded image\n\ndef set_loss_weight(loss_weight_type, weight_function=None, degraded_image=None, x_0_hat=None):\n    # weight function is a string divided into \"function,value0,value1,...\"\n    if isinstance(weight_function, str):\n        str_parts = weight_function.split(\",\")\n        function_str = str_parts[0]\n\n        if len(str_parts) > 1:\n            value = np.asarray(str_parts[1:]).astype(float)\n            value = value.item() if value.shape[0] == 1 else value\n\n    else:\n        function_str = 'none'\n\n    if loss_weight_type == 'none' or loss_weight_type is None:\n        loss_weight = 1\n\n    # try to multiply by the depth, the reason is to make the gradients of the far area larger since the\n    # prediction from the u-net got close to zero at those areas\n    elif loss_weight_type == 'depth':\n\n        depth_tmp = x_0_hat.detach()[:, 3, :, :].unsqueeze(1)\n        loss_weight = convert_depth(depth=depth_tmp, depth_type=function_str, value=value)\n\n    else:\n        raise NotImplementedError\n\n    return loss_weight\n\n\n# %% create histogram image\n\ndef color_histogram(img, title=None):\n    \"\"\"\n    :param img: image should be tensor (c, h, w) between values [0.,1.]\n    :return: tensor image of histogram (c, h, w) between values [0.,1.]\n    \"\"\"\n\n    img = torch.clamp(img, min=0., max=1.)\n\n    colors = (\"red\", \"green\", \"blue\")\n    img_np = (img * 255).to(torch.uint8).permute(1, 2, 0).numpy()\n    # get the dimensions\n    ypixels, xpixels, bands = img_np.shape\n\n    # get the size in inches\n    dpi = plt.rcParams['figure.dpi']\n    xinch = xpixels / dpi\n    yinch = ypixels / dpi\n\n    fig = plt.figure(figsize=(xinch, yinch))\n    plt.xlim([-5, 260])\n\n    for channel_id, color in enumerate(colors):\n        histogram, bin_edges = np.histogram(img_np[:, :, channel_id], bins=256, range=(0, 256))\n        plt.plot(bin_edges[0:-1], histogram, color=color)\n\n    plt.grid()\n    plt.yticks(rotation=45, ha='right', fontsize=7)\n    if title is not None:\n        plt.title(str(title))\n\n    canvas = fig.canvas\n    canvas.draw()  # Draw the canvas, cache the renderer\n    hist_image_flat = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')  # (H * W * 3,)\n    # NOTE: reversed converts (W, H) from get_width_height to (H, W)\n    hist_np = hist_image_flat.reshape(*reversed(canvas.get_width_height()), 3)  # (H, W, 3)\n    hist_tensor = tvtf.to_tensor(Image.fromarray(hist_np))\n    plt.close(fig)\n\n    return hist_tensor\n\n\n# %% save depth tensor into rgb with colormap (instead of grayscale)\n\ndef depth_tensor_to_color_image(tensor_image, colormap='viridis'):\n    cm = plt.get_cmap(colormap)\n\n    if len(tensor_image.shape) == 4:\n        tensor_image = tensor_image.squeeze()\n\n    if len(tensor_image.shape) == 3:\n        tensor_image = tensor_image[0]\n\n    assert len(tensor_image.shape) == 2\n\n    # color the gray scale image\n    im_np = cm(tensor_image.numpy())\n    depth_im_ii = torch.tensor(im_np[:, :, 0:3]).permute(2, 0, 1)\n\n    return depth_im_ii\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:14:51.506794Z","iopub.execute_input":"2024-11-13T09:14:51.507263Z","iopub.status.idle":"2024-11-13T09:14:51.629008Z","shell.execute_reply.started":"2024-11-13T09:14:51.507213Z","shell.execute_reply":"2024-11-13T09:14:51.628271Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"**GAUSSIAN DIFFUSION**","metadata":{}},{"cell_type":"code","source":"import math\nimport os\nfrom os.path import join as pjoin\nfrom functools import partial\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torchvision.utils import make_grid\nimport torchvision.transforms.functional as tvtf\n\n__SAMPLER__ = {}\n\n\ndef register_sampler(name: str):\n    def wrapper(cls):\n        if __SAMPLER__.get(name, None):\n            raise NameError(f\"Name {name} is already registered!\")\n        __SAMPLER__[name] = cls\n        return cls\n\n    return wrapper\n\n\ndef get_sampler(name: str):\n    if __SAMPLER__.get(name, None) is None:\n        raise NameError(f\"Name {name} is not defined!\")\n    return __SAMPLER__[name]\n\n\ndef create_sampler(sampler,\n                   steps,\n                   noise_schedule,\n                   model_mean_type,\n                   model_var_type,\n                   dynamic_threshold,\n                   clip_denoised,\n                   rescale_timesteps,\n                   timestep_respacing=\"\",\n                   **kwargs):\n    sampler = get_sampler(name=sampler)\n\n    annealing_time = kwargs.get('annealing_time', False)\n    betas = get_named_beta_schedule(noise_schedule, steps)\n    if not timestep_respacing:\n        timestep_respacing = [steps]\n\n    return sampler(use_timesteps=space_timesteps(steps, timestep_respacing),\n                   betas=betas,\n                   model_mean_type=model_mean_type,\n                   model_var_type=model_var_type,\n                   dynamic_threshold=dynamic_threshold,\n                   clip_denoised=clip_denoised,\n                   rescale_timesteps=rescale_timesteps,\n                   annealing_time=annealing_time)\n\n\nclass GaussianDiffusion:\n    def __init__(self,\n                 betas,\n                 model_mean_type,\n                 model_var_type,\n                 dynamic_threshold,\n                 clip_denoised,\n                 rescale_timesteps,\n                 **kwargs):\n\n        # use float64 for accuracy.\n        betas = np.array(betas, dtype=np.float64)\n        self.betas = betas\n        assert self.betas.ndim == 1, \"betas must be 1-D\"\n        assert (0 < self.betas).all() and (self.betas <= 1).all(), \"betas must be in (0..1]\"\n\n        self.num_timesteps = int(self.betas.shape[0])\n        self.rescale_timesteps = rescale_timesteps\n\n        alphas = 1.0 - self.betas\n        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        self.posterior_variance = (\n                betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        )\n        # log calculation clipped because the posterior variance is 0 at the\n        # beginning of the diffusion chain.\n        self.posterior_log_variance_clipped = np.log(\n            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n        )\n        self.posterior_mean_coef1 = (\n                betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        )\n        self.posterior_mean_coef2 = (\n                (1.0 - self.alphas_cumprod_prev)\n                * np.sqrt(alphas)\n                / (1.0 - self.alphas_cumprod)\n        )\n\n        self.mean_processor = get_mean_processor(model_mean_type,\n                                                 betas=betas,\n                                                 dynamic_threshold=dynamic_threshold,\n                                                 clip_denoised=clip_denoised)\n\n        self.var_processor = get_var_processor(model_var_type,\n                                               betas=betas)\n\n    def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n\n        mean = extract_and_expand(self.sqrt_alphas_cumprod, t, x_start) * x_start\n        variance = extract_and_expand(1.0 - self.alphas_cumprod, t, x_start)\n        log_variance = extract_and_expand(self.log_one_minus_alphas_cumprod, t, x_start)\n\n        return mean, variance, log_variance\n\n    def q_sample(self, x_start, t):\n        \"\"\"\n        Diffuse the data for a given number of diffusion steps.\n\n        In other words, sample from q(x_t | x_0).\n\n        :param x_start: the initial data batch.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :param noise: if specified, the split-out normal noise.\n        :return: A noisy version of x_start.\n        \"\"\"\n        noise = torch.randn_like(x_start)\n        assert noise.shape == x_start.shape\n\n        coef1 = extract_and_expand(self.sqrt_alphas_cumprod, t, x_start)\n        coef2 = extract_and_expand(self.sqrt_one_minus_alphas_cumprod, t, x_start)\n\n        return coef1 * x_start + coef2 * noise\n\n    def q_posterior_mean_variance(self, x_start, x_t, t):\n        \"\"\"\n        Compute the mean and variance of the diffusion posterior:\n\n            q(x_{t-1} | x_t, x_0)\n\n        \"\"\"\n        assert x_start.shape == x_t.shape\n        coef1 = extract_and_expand(self.posterior_mean_coef1, t, x_start)\n        coef2 = extract_and_expand(self.posterior_mean_coef2, t, x_t)\n        posterior_mean = coef1 * x_start + coef2 * x_t\n        posterior_variance = extract_and_expand(self.posterior_variance, t, x_t)\n        posterior_log_variance_clipped = extract_and_expand(self.posterior_log_variance_clipped, t, x_t)\n\n        assert (\n                posterior_mean.shape[0]\n                == posterior_variance.shape[0]\n                == posterior_log_variance_clipped.shape[0]\n                == x_start.shape[0]\n        )\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_sample_loop(self,\n                      model,\n                      x_start,\n                      measurement,\n                      measurement_cond_fn,\n                      record,\n                      save_root,\n                      pretrain_model=None,\n                      image_idx=None,\n                      record_every=150,\n                      rgb_guidance=False,\n                      sample_pattern=None,\n                      **kwargs):\n        \"\"\"\n        The function used for sampling from noise.\n        \"\"\"\n\n        img = x_start\n        device = x_start.device\n        global_iteration = kwargs.get(\"global_iteration\", False)\n        original_file_name = kwargs.get(\"original_file_name\", \"image_0\")\n        save_grids_path = kwargs.get(\"save_grids_path\", None)\n\n        time_val_list = []\n        loss_process = []\n\n        if record:\n            rgb_record_list = []\n            depth_record_list = []\n\n        total_steps = self.num_timesteps\n        pbar = tqdm(list(range(total_steps))[::-1])\n\n        # loop over the timestep\n        for idx in pbar:\n\n            time = torch.tensor([idx] * img.shape[0], device=device)\n            time_val_list.append(time.cpu().item())\n\n            # flag (bool) for non guidance\n            guidance_flag = (sample_pattern['pattern'] == 'original') or \\\n                            (sample_pattern['pattern'] is None) or \\\n                            (sample_pattern['start_guidance'] * self.num_timesteps >= time >= sample_pattern[\n                                'stop_guidance'] * self.num_timesteps)\n\n            # setting the alternate len (M from the gibbsDDRM paper)\n            alternate_len = set_alternate_length(sample_pattern, idx, self.num_timesteps)\n\n            # for osmosis use alternate_len=1, means - no alternating\n            for alternate_ii in range(alternate_len):\n\n                img.requires_grad = True if guidance_flag else False\n\n                if rgb_guidance:\n                    out = self.p_sample(x=img, t=time, model=model)\n\n                else:\n                    # \"clean\" the noise with the unet\n                    out = self.p_mean_variance(model=model, x=img, t=time)\n                    out['sample'] = out['mean']\n\n                # there is no use of the noisy measurement, do we need it? I don't know yet\n                noisy_measurement = self.q_sample(measurement, t=time)\n\n                # Give condition. -> guiding\n                if pretrain_model == 'osmosis' and not rgb_guidance:\n\n                    # check if there is a sampling method and check the idx to check if to freeze phis\n                    freeze_phi = is_freeze_phi(sample_pattern, idx, self.num_timesteps)\n\n                    if guidance_flag:\n\n                        # conditioning function (guidance)\n                        img, loss, variable_dict, gradients, aux_loss = \\\n                            measurement_cond_fn(x_t=out['sample'],\n                                                measurement=measurement,\n                                                noisy_measurement=noisy_measurement,\n                                                x_prev=img,\n                                                x_0_hat=out['pred_xstart'],\n                                                freeze_phi=freeze_phi,\n                                                time_index=float(idx) / self.num_timesteps)\n\n                    else:\n                        # no guidance\n                        img = out['sample']\n\n                    # sampling new img after guidance\n                    noise = torch.randn_like(img, device=img.device)\n                    if time != 0:  # no noise when t == 0\n                        img += torch.exp(0.5 * out['log_variance']) * noise\n\n                    # detach result from graph, for the next iteration\n                    img.detach_()\n\n                    # update pbar for the last alternating process\n                    if alternate_ii == (alternate_len - 1):\n\n                        loss_process.append(loss[0].item())\n                        # print and log values\n                        pbar_print_dictionary = {}\n                        pbar_print_dictionary['time'] = time.cpu().tolist()\n                        pbar_print_dictionary['loss'] = loss\n                        # print auxiliary loss to the pbar\n                        if aux_loss is not None:\n                            pbar_print_dictionary['aux'] = np.round(\n                                [ii.item() for ii in list(aux_loss.values())], decimals=4)\n\n                        # print variables to pbar\n                        for key_ii, value_ii in variable_dict.items():\n                            current_var_value = np.round(value_ii.cpu().detach().squeeze().tolist(), decimals=3)\n                            # in case the variable is a matrix\n                            if len(current_var_value.shape) > 1:\n                                current_var_value = \\\n                                    np.round([current_var_value.mean(), current_var_value.std()], decimals=3)\n                            pbar_print_dictionary[key_ii] = current_var_value\n\n                        # print the pbar\n                        pbar.set_postfix(pbar_print_dictionary, refresh=False)\n\n                # almost original dps code - rgb_guidance\n                else:\n                    img, loss = measurement_cond_fn(x_t=out['sample'],\n                                                    measurement=measurement,\n                                                    noisy_measurement=noisy_measurement,\n                                                    x_prev=img,\n                                                    x_0_hat=out['pred_xstart'])\n                    img = img.detach_()\n                    pbar.set_postfix({'loss': loss.detach().cpu().item()}, refresh=False)\n\n                # save the images during the diffusion process\n                if record and (alternate_ii == (alternate_len - 1)) and \\\n                        ((idx % record_every == 0) or (idx == 0) or (idx == 999)):\n                    # the RGBD image\n                    mid_x_0_pred_tmp = out['pred_xstart'].detach().cpu()\n\n                    # split into RGB and Depth images\n                    rgb_record_tmp = 0.5 * (mid_x_0_pred_tmp[0, 0:3, :, :] + 1)\n                    rgb_record_tmp_clip = torch.clamp(rgb_record_tmp, 0, 1)\n\n                    # Depth\n                    depth_record_tmp = mid_x_0_pred_tmp[:, 3, :, :]\n                    # percentile + min max norm for the depth image\n                    depth_record_tmp_pmm = min_max_norm_range_percentile(depth_record_tmp, percent_low=0.05,\n                                                                                percent_high=0.99)\n                    depth_record_tmp_pmm_color = depth_tensor_to_color_image(depth_record_tmp_pmm)\n\n                    rgb_record_list.append(rgb_record_tmp_clip)\n                    depth_record_list.append(depth_record_tmp_pmm_color)\n\n        # save the recorded images\n        if record and (save_grids_path is not None):\n            # save rgb and depth information - images are clipped, depth is percentiled + min-max normalized\n            mid_grid = make_grid(rgb_record_list + depth_record_list, nrow=len(rgb_record_list))\n            mid_grid_pil = tvtf.to_pil_image(mid_grid)\n            mid_grid_pil.save(pjoin(save_grids_path, f'{original_file_name}_process.png'))\n\n        # return the relevant things\n        if pretrain_model == 'osmosis' and not rgb_guidance:\n            return img, variable_dict, loss, out['pred_xstart'].detach().cpu()\n\n        else:\n            return img\n\n    def p_sample(self, model, x, t):\n        raise NotImplementedError\n\n    def p_mean_variance(self, model, x, t):\n        model_output = model(x, self._scale_timesteps(t))\n\n        # In the case of \"learned\" variance, model will give twice channels.\n        if model_output.shape[1] == 2 * x.shape[1]:\n            model_output, model_var_values = torch.split(model_output, x.shape[1], dim=1)\n        else:\n            # The name of variable is wrong. \n            # This will just provide shape information, and \n            # will not be used for calculating something important in variance.\n            model_var_values = model_output\n\n        model_mean, pred_xstart = self.mean_processor.get_mean_and_xstart(x, t, model_output)\n        model_variance, model_log_variance = self.var_processor.get_variance(model_var_values, t)\n\n        assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n\n        return {'mean': model_mean,\n                'variance': model_variance,\n                'log_variance': model_log_variance,\n                'pred_xstart': pred_xstart}\n\n    def _scale_timesteps(self, t):\n        if self.rescale_timesteps:\n            return t.float() * (1000.0 / self.num_timesteps)\n        return t\n\n\ndef space_timesteps(num_timesteps, section_counts):\n    \"\"\"\n    Create a list of timesteps to use from an original diffusion process,\n    given the number of timesteps we want to take from equally-sized portions\n    of the original process.\n    For example, if there's 300 timesteps and the section counts are [10,15,20]\n    then the first 100 timesteps are strided to be 10 timesteps, the second 100\n    are strided to be 15 timesteps, and the final 100 are strided to be 20.\n    If the stride is a string starting with \"ddim\", then the fixed striding\n    from the DDIM paper is used, and only one section is allowed.\n    :param num_timesteps: the number of diffusion steps in the original\n                          process to divide up.\n    :param section_counts: either a list of numbers, or a string containing\n                           comma-separated numbers, indicating the step count\n                           per section. As a special case, use \"ddimN\" where N\n                           is a number of steps to use the striding from the\n                           DDIM paper.\n    :return: a set of diffusion steps from the original process to use.\n    \"\"\"\n    if isinstance(section_counts, str):\n        if section_counts.startswith(\"ddim\"):\n            desired_count = int(section_counts[len(\"ddim\"):])\n            for i in range(1, num_timesteps):\n                if len(range(0, num_timesteps, i)) == desired_count:\n                    return set(range(0, num_timesteps, i))\n            raise ValueError(\n                f\"cannot create exactly {num_timesteps} steps with an integer stride\"\n            )\n        section_counts = [int(x) for x in section_counts.split(\",\")]\n    elif isinstance(section_counts, int):\n        section_counts = [section_counts]\n\n    size_per = num_timesteps // len(section_counts)\n    extra = num_timesteps % len(section_counts)\n    start_idx = 0\n    all_steps = []\n    for i, section_count in enumerate(section_counts):\n        size = size_per + (1 if i < extra else 0)\n        if size < section_count:\n            raise ValueError(\n                f\"cannot divide section of {size} steps into {section_count}\"\n            )\n        if section_count <= 1:\n            frac_stride = 1\n        else:\n            frac_stride = (size - 1) / (section_count - 1)\n        cur_idx = 0.0\n        taken_steps = []\n        for _ in range(section_count):\n            taken_steps.append(start_idx + round(cur_idx))\n            cur_idx += frac_stride\n        all_steps += taken_steps\n        start_idx += size\n    return set(all_steps)\n\n\nclass SpacedDiffusion(GaussianDiffusion):\n    \"\"\"\n    A diffusion process which can skip steps in a base diffusion process.\n    :param use_timesteps: a collection (sequence or set) of timesteps from the\n                          original diffusion process to retain.\n    :param kwargs: the kwargs to create the base diffusion process.\n    \"\"\"\n\n    def __init__(self, use_timesteps, **kwargs):\n        self.use_timesteps = set(use_timesteps)\n        self.timestep_map = []\n        self.original_num_steps = len(kwargs[\"betas\"])\n\n        base_diffusion = GaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa\n        last_alpha_cumprod = 1.0\n        new_betas = []\n        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n            if i in self.use_timesteps:\n                new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n                last_alpha_cumprod = alpha_cumprod\n                self.timestep_map.append(i)\n        kwargs[\"betas\"] = np.array(new_betas)\n        super().__init__(**kwargs)\n\n    def p_mean_variance(self, model, *args, **kwargs):  # pylint: disable=signature-differs\n        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n\n    def training_losses(self, model, *args, **kwargs):  # pylint: disable=signature-differs\n        return super().training_losses(self._wrap_model(model), *args, **kwargs)\n\n    def condition_mean(self, cond_fn, *args, **kwargs):\n        return super().condition_mean(self._wrap_model(cond_fn), *args, **kwargs)\n\n    def condition_score(self, cond_fn, *args, **kwargs):\n        return super().condition_score(self._wrap_model(cond_fn), *args, **kwargs)\n\n    def _wrap_model(self, model):\n        if isinstance(model, _WrappedModel):\n            return model\n        return _WrappedModel(\n            model, self.timestep_map, self.rescale_timesteps, self.original_num_steps\n        )\n\n    def _scale_timesteps(self, t):\n        # Scaling is done by the wrapped model.\n        return t\n\n\nclass _WrappedModel:\n    def __init__(self, model, timestep_map, rescale_timesteps, original_num_steps):\n        self.model = model\n        self.timestep_map = timestep_map\n        self.rescale_timesteps = rescale_timesteps\n        self.original_num_steps = original_num_steps\n\n    def __call__(self, x, ts, **kwargs):\n        map_tensor = torch.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)\n        new_ts = map_tensor[ts]\n        if self.rescale_timesteps:\n            new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\n        return self.model(x, new_ts, **kwargs)\n\n\n@register_sampler(name='ddpm')\nclass DDPM(SpacedDiffusion):\n    def p_sample(self, model, x, t):\n        out = self.p_mean_variance(model, x, t)\n        sample = out['mean']\n\n        noise = torch.randn_like(x)\n        if t[0] != 0:  # no noise when t == 0\n            sample += torch.exp(0.5 * out['log_variance']) * noise\n\n        return {'sample': sample, 'pred_xstart': out['pred_xstart']}\n\n\n@register_sampler(name='ddim')\nclass DDIM(SpacedDiffusion):\n    def p_sample(self, model, x, t, eta=0.0):\n        out = self.p_mean_variance(model, x, t)\n\n        eps = self.predict_eps_from_x_start(x, t, out['pred_xstart'])\n\n        alpha_bar = extract_and_expand(self.alphas_cumprod, t, x)\n        alpha_bar_prev = extract_and_expand(self.alphas_cumprod_prev, t, x)\n        sigma = (\n                eta\n                * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n                * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n        )\n        # Equation 12.\n        noise = torch.randn_like(x)\n        mean_pred = (\n                out[\"pred_xstart\"] * torch.sqrt(alpha_bar_prev)\n                + torch.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n        )\n\n        sample = mean_pred\n        if t != 0:\n            sample += sigma * noise\n\n        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n\n    def predict_eps_from_x_start(self, x_t, t, pred_xstart):\n        coef1 = extract_and_expand(self.sqrt_recip_alphas_cumprod, t, x_t)\n        coef2 = extract_and_expand(self.sqrt_recipm1_alphas_cumprod, t, x_t)\n        return (coef1 * x_t - pred_xstart) / coef2\n\n\n# =================\n# Helper functions\n# =================\n\ndef get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of\n        # diffusion steps.\n        scale = 1000 / num_diffusion_timesteps\n        beta_start = scale * 0.0001\n        beta_end = scale * 0.02\n        return np.linspace(\n            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n        )\n    elif schedule_name == \"cosine\":\n        return betas_for_alpha_bar(\n            num_diffusion_timesteps,\n            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n        )\n    else:\n        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n\n\ndef betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.\n    \"\"\"\n    betas = []\n    for i in range(num_diffusion_timesteps):\n        t1 = i / num_diffusion_timesteps\n        t2 = (i + 1) / num_diffusion_timesteps\n        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n    return np.array(betas)\n\n\n# ================\n# Helper function\n# ================\n\ndef extract_and_expand(array, time, target):\n    array = torch.from_numpy(array).to(target.device)[time].float()\n    while array.ndim < target.ndim:\n        array = array.unsqueeze(-1)\n    return array.expand_as(target)\n\n\ndef expand_as(array, target):\n    if isinstance(array, np.ndarray):\n        array = torch.from_numpy(array)\n    elif isinstance(array, np.float):\n        array = torch.tensor([array])\n\n    while array.ndim < target.ndim:\n        array = array.unsqueeze(-1)\n\n    return array.expand_as(target).to(target.device)\n\n\ndef _extract_into_tensor(arr, timesteps, broadcast_shape):\n    \"\"\"\n    Extract values from a 1-D numpy array for a batch of indices.\n\n    :param arr: the 1-D numpy array.\n    :param timesteps: a tensor of indices into the array to extract.\n    :param broadcast_shape: a larger shape of K dimensions with the batch\n                            dimension equal to the length of timesteps.\n    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n    \"\"\"\n    res = torch.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n    while len(res.shape) < len(broadcast_shape):\n        res = res[..., None]\n    return res.expand(broadcast_shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:18:24.637323Z","iopub.execute_input":"2024-11-13T09:18:24.637679Z","iopub.status.idle":"2024-11-13T09:18:24.721803Z","shell.execute_reply.started":"2024-11-13T09:18:24.637645Z","shell.execute_reply":"2024-11-13T09:18:24.720779Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"\"\"\"\nThis module handles task-dependent operations\n\"\"\"\n\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nfrom torchvision import torch\n\n# =================\n# Operation classes\n# =================\n\n__OPERATOR__ = {}\n\n\ndef register_operator(name: str):\n    def wrapper(cls):\n        if __OPERATOR__.get(name, None):\n            raise NameError(f\"Name {name} is already registered!\")\n        __OPERATOR__[name] = cls\n\n        return cls\n\n    return wrapper\n\n\ndef get_operator(name: str, **kwargs):\n    if __OPERATOR__.get(name, None) is None:\n        raise NameError(f\"Name {name} is not defined.\")\n\n    operator = __OPERATOR__[name](**kwargs)\n    operator.__name__ = name\n\n    # return __OPERATOR__[name](**kwargs)\n    return operator\n\n\nclass LinearOperator(ABC):\n    @abstractmethod\n    def forward(self, data, **kwargs):\n        # calculate A * X\n        pass\n\n    @abstractmethod\n    def transpose(self, data, **kwargs):\n        # calculate A^T * X\n        pass\n\n    def ortho_project(self, data, **kwargs):\n        # calculate (I - A^T * A)X\n        return data - self.transpose(self.forward(data, **kwargs), **kwargs)\n\n    def project(self, data, measurement, **kwargs):\n        # calculate (I - A^T * A)Y - AX\n        return self.ortho_project(measurement, **kwargs) - self.forward(data, **kwargs)\n\n\n@register_operator(name='noise')\nclass DenoiseOperator(LinearOperator):\n    def __init__(self, device, batch_size=1, **kargs):\n        self.device = device\n        self.batch_size = batch_size\n\n    def forward(self, data, **kargs):\n        return data\n\n    def transpose(self, data):\n        return data\n\n    def ortho_project(self, data):\n        return data\n\n    def project(self, data):\n        return data\n\n\n@register_operator(name='rgb_guidance')\nclass RGBGuidanceOperator(LinearOperator):\n    def __init__(self, device, batch_size=1, **kargs):\n        self.device = device\n        self.batch_size = batch_size\n\n    def forward(self, data, **kargs):\n        return data\n\n    def transpose(self, data):\n        return data\n\n    def ortho_project(self, data):\n        return data\n\n    def project(self, data):\n        return data\n\n\n# osmosis - learnable Operator\nclass LearnableOperator(ABC):\n\n    @abstractmethod\n    def forward(self, data, **kwargs):\n        pass\n\n\n@register_operator(name='haze_physical')\nclass HazePhysicalOperator(LearnableOperator):\n    def __init__(self, device, phi_ab, phi_inf, phi_ab_eta=1e-5, phi_inf_eta=1e-5,\n                 phi_ab_learn_flag=True, phi_inf_learn_flag=True,\n                 batch_size=1, **kwargs):\n\n        self.device = device\n        self.depth_type = kwargs.get(\"depth_type\", None)\n        tmp_value = kwargs.get(\"value\", None)\n        self.value = get_depth_value(tmp_value)\n\n        # initialization values\n        self.phi_ab = torch.tensor(float(phi_ab)).to(device)\n        self.phi_ab = self.phi_ab.repeat(batch_size, 1).unsqueeze(-1).unsqueeze(-1)\n\n        self.phi_inf = torch.tensor(np.fromstring(phi_inf, dtype=float, sep=','), dtype=torch.float, device=device)\n        self.phi_inf = self.phi_inf.repeat(batch_size, 1).unsqueeze(-1).unsqueeze(-1)\n\n        self.phi_ab_learn_flag = phi_ab_learn_flag\n        self.phi_inf_learn_flag = phi_inf_learn_flag\n\n        # coefficients for the Gradient descend step size\n        self.phi_ab_eta = float(phi_ab_eta) if phi_ab_learn_flag else float(0)\n        self.phi_inf_eta = float(phi_inf_eta) if phi_inf_learn_flag else float(0)\n\n        # set optimizer\n        optimizer = kwargs.get(\"optimizer\", None)\n        self.optimizer = get_optimizer(optimizer_name=optimizer,\n                                              model_parameters=[{'params': self.phi_ab, \"lr\": self.phi_ab_eta},\n                                                                {'params': self.phi_inf, \"lr\": self.phi_inf_eta}])\n\n    def forward(self, data, **kwargs):\n\n        # split into rgb and depth\n        rgb = data[:, 0:-1, :, :]\n        rgb_norm = 0.5 * (rgb + 1)\n        depth_tmp = data[:, -1, :, :].unsqueeze(1)\n\n        # convert depth to relevant coordinates\n        depth = convert_depth(depth=depth_tmp, depth_type=self.depth_type, value=self.value)\n\n        # the underwater image formation model\n        uw_image = rgb_norm * torch.exp(-self.phi_ab * depth) + self.phi_inf * (1 - torch.exp(-self.phi_ab * depth))\n\n        return uw_image\n\n    def optimize(self, **kwargs):\n\n        freeze_phi = kwargs.get(\"freeze_phi\", False)\n\n        # update only part of the variables - in this case: self.optimizer == \"GD\"\n        update_phi_ab = self.phi_ab.requires_grad\n        update_phi_inf = self.phi_inf.requires_grad\n\n        # when freeze_phi is True that means no optimization is required\n        if not freeze_phi:\n\n            # no optimizer was specified - GD is the default\n            if self.optimizer is None or self.optimizer == \"GD\" or self.optimizer == \"\":\n\n                # classic gradient descend\n                with torch.no_grad():\n                    if update_phi_ab:\n                        self.phi_ab.add_(self.phi_ab.grad, alpha=-self.phi_ab_eta)\n                    if update_phi_inf:\n                        self.phi_inf.add_(self.phi_inf.grad, alpha=-self.phi_inf_eta)\n                # zero the gradients so they will not accumulate\n                if update_phi_ab:\n                    self.phi_ab.grad.zero_()\n                if update_phi_inf:\n                    self.phi_inf.grad.zero_()\n\n            # optimizer was specified\n            else:\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n\n        # return self.beta.detach(), self.b_inf.detach()\n        return {'phi_ab': self.phi_ab.detach(), 'phi_inf': self.phi_inf.detach()}\n\n    def get_variable_gradients(self, **kwargs):\n\n        grad_enable_dict = {\"phi_ab\": self.phi_ab.requires_grad,\n                            \"phi_inf\": self.phi_inf.requires_grad}\n\n        return grad_enable_dict\n\n    def set_variable_gradients(self, value=None, **kwargs):\n\n        if value is None:\n            raise ValueError(\"A value should be specified (True or False for general or dictionary)\")\n\n        if isinstance(value, dict):\n            self.phi_ab.requires_grad_(value[\"phi_ab\"])\n            self.phi_inf.requires_grad_(value[\"phi_inf\"])\n        else:\n            self.phi_ab.requires_grad_(value)\n            self.phi_inf.requires_grad_(value)\n\n    def get_variable_list(self, **kwargs):\n\n        return [self.phi_ab, self.phi_inf]\n\n\n@register_operator(name='underwater_physical_revised')\nclass UnderWaterPhysicalRevisedOperator(LearnableOperator):\n    def __init__(self, device, phi_a, phi_b, phi_inf,\n                 phi_a_eta=1e-5, phi_b_eta=1e-5, phi_inf_eta=1e-5,\n                 phi_a_learn_flag=True, phi_b_learn_flag=True, phi_inf_learn_flag=True,\n                 batch_size=1, **kwargs):\n\n        self.device = device\n\n        self.depth_type = kwargs.get(\"depth_type\", None)\n        tmp_value = kwargs.get(\"value\", None)\n        self.value = get_depth_value(tmp_value)\n\n        # initialization values\n        self.phi_a = torch.tensor(np.fromstring(phi_a, dtype=float, sep=','), dtype=torch.float, device=device)\n        self.phi_a = self.phi_a.repeat(batch_size, 1).unsqueeze(-1).unsqueeze(-1)\n\n        self.phi_b = torch.tensor(np.fromstring(phi_b, dtype=float, sep=','), dtype=torch.float, device=device)\n        self.phi_b = self.phi_b.repeat(batch_size, 1).unsqueeze(-1).unsqueeze(-1)\n\n        self.phi_inf = torch.tensor(np.fromstring(phi_inf, dtype=float, sep=','), dtype=torch.float, device=device)\n        self.phi_inf = self.phi_inf.repeat(batch_size, 1).unsqueeze(-1).unsqueeze(-1)\n\n        # learning flags\n        self.phi_a_learn_flag = phi_a_learn_flag\n        self.phi_b_learn_flag = phi_b_learn_flag\n        self.phi_inf_learn_flag = phi_inf_learn_flag\n\n        # coefficients for the Gradient descend step size\n        self.phi_a_eta = float(phi_a_eta) if phi_a_learn_flag else float(0)\n        self.phi_b_eta = float(phi_b_eta) if phi_b_learn_flag else float(0)\n        self.phi_inf_eta = float(phi_inf_eta) if phi_inf_learn_flag else float(0)\n\n        # set optimizer\n        optimizer = kwargs.get(\"optimizer\", None)\n        self.optimizer = get_optimizer(optimizer_name=optimizer,\n                                              model_parameters=[{'params': self.phi_a, \"lr\": self.phi_a_eta},\n                                                                {'params': self.phi_b, \"lr\": self.phi_b_eta},\n                                                                {'params': self.phi_inf, \"lr\": self.phi_inf_eta}])\n\n    def forward(self, data, **kwargs):\n\n        # split into rgb and depth\n        rgb = data[:, 0:-1, :, :]\n        rgb_norm = 0.5 * (rgb + 1)\n        depth_tmp = data[:, -1, :, :].unsqueeze(1)\n\n        # convert depth to relevant coordinates\n        depth = convert_depth(depth=depth_tmp, depth_type=self.depth_type, value=self.value)\n\n        # the underwater image formation model\n        uw_image = rgb_norm * torch.exp(-self.phi_a * depth) + self.phi_inf * (1 - torch.exp(-self.phi_b * depth))\n\n        return uw_image\n\n    def optimize(self, **kwargs):\n\n        freeze_phi = kwargs.get(\"freeze_phi\", False)\n\n        # update only part of the variables - in this case: self.optimizer == \"GD\"\n        update_phi_a = self.phi_a.requires_grad\n        update_phi_b = self.phi_b.requires_grad\n        update_phi_inf = self.phi_inf.requires_grad\n\n        # when freeze_phi is True that means no optimization is required\n        if not freeze_phi:\n\n            # no optimizer was specified - GD is the default\n            if self.optimizer is None or self.optimizer == \"GD\" or self.optimizer == \"\":\n\n                # classic gradient descend\n                with torch.no_grad():\n                    if update_phi_a:\n                        self.phi_a.add_(self.phi_a.grad, alpha=-self.phi_a_eta)\n                    if update_phi_b:\n                        self.phi_b.add_(self.phi_b.grad, alpha=-self.phi_b_eta)\n                    if update_phi_inf:\n                        self.phi_inf.add_(self.phi_inf.grad, alpha=-self.phi_inf_eta)\n\n                # zero the gradients so they will not accumulate\n                if update_phi_a:\n                    self.phi_a.grad.zero_()\n                if update_phi_b:\n                    self.phi_b.grad.zero_()\n                if update_phi_inf:\n                    self.phi_inf.grad.zero_()\n\n            else:\n\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n\n        return {'phi_a': self.phi_a.detach(), 'phi_b': self.phi_b.detach(), 'phi_inf': self.phi_inf.detach()}\n\n    def get_variable_gradients(self, **kwargs):\n\n        grad_enable_dict = {\"phi_a\": self.phi_a.requires_grad,\n                            \"phi_b\": self.phi_b.requires_grad,\n                            \"phi_inf\": self.phi_inf.requires_grad}\n\n        return grad_enable_dict\n\n    def set_variable_gradients(self, value=None, **kwargs):\n\n        if value is None:\n            raise ValueError(\"A value should be specified (True or False for general or dictionary)\")\n\n        if isinstance(value, dict):\n            self.phi_a.requires_grad_(value[\"phi_a\"])\n            self.phi_b.requires_grad_(value[\"phi_b\"])\n            self.phi_inf.requires_grad_(value[\"phi_inf\"])\n        else:\n            self.phi_a.requires_grad_(value)\n            self.phi_b.requires_grad_(value)\n            self.phi_inf.requires_grad_(value)\n\n    def get_variable_list(self, **kwargs):\n\n        return [self.phi_a, self.phi_b, self.phi_inf]\n\n\n@register_operator(name='underwater_physical')\nclass UnderWaterPhysicalOperator(LearnableOperator):\n    def __init__(self, device, phi_ab, phi_inf, phi_ab_eta=1e-5, phi_inf_eta=1e-5,\n                 phi_ab_learn_flag=True, phi_inf_learn_flag=True,\n                 batch_size=1, **kwargs):\n\n        self.device = device\n        self.depth_type = kwargs.get(\"depth_type\", None)\n        tmp_value = kwargs.get(\"value\", None)\n        self.value = get_depth_value(tmp_value)\n\n        # initialization values\n        self.phi_ab = torch.tensor(np.fromstring(phi_ab, dtype=float, sep=','), dtype=torch.float, device=device)\n        self.phi_ab = self.phi_ab.repeat(batch_size, 1).unsqueeze(-1).unsqueeze(-1)\n\n        self.phi_inf = torch.tensor(np.fromstring(phi_inf, dtype=float, sep=','), dtype=torch.float, device=device)\n        self.phi_inf = self.phi_inf.repeat(batch_size, 1).unsqueeze(-1).unsqueeze(-1)\n\n        self.phi_ab_learn_flag = phi_ab_learn_flag\n        self.phi_inf_learn_flag = phi_inf_learn_flag\n\n        # coefficients for the Gradient descend step size\n        self.phi_ab_eta = float(phi_ab_eta) if phi_ab_learn_flag else float(0)\n        self.phi_inf_eta = float(phi_inf_eta) if phi_inf_learn_flag else float(0)\n\n        # set optimizer\n        optimizer = kwargs.get(\"optimizer\", None)\n        self.optimizer = get_optimizer(optimizer_name=optimizer,\n                                              model_parameters=[{'params': self.phi_ab, \"lr\": self.phi_ab_eta},\n                                                                {'params': self.phi_inf, \"lr\": self.phi_inf_eta}])\n\n    def forward(self, data, **kwargs):\n\n        # split into rgb and depth\n        rgb = data[:, 0:-1, :, :]\n        rgb_norm = 0.5 * (rgb + 1)\n        depth_tmp = data[:, -1, :, :].unsqueeze(1)\n\n        # convert depth to relevant coordinates\n        depth = convert_depth(depth=depth_tmp, depth_type=self.depth_type, value=self.value)\n\n        # the underwater image formation model\n        uw_image = rgb_norm * torch.exp(-self.phi_ab * depth) + self.phi_inf * (1 - torch.exp(-self.phi_ab * depth))\n\n        return uw_image\n\n    def optimize(self, **kwargs):\n\n        freeze_phi = kwargs.get(\"freeze_phi\", False)\n\n        # update only part of the variables - in this case: self.optimizer == \"GD\"\n        update_phi_ab = self.phi_ab.requires_grad\n        update_phi_inf = self.phi_inf.requires_grad\n\n        # when freeze_phi is True that means no optimization is required\n        if not freeze_phi:\n\n            # no optimizer was specified - GD is the default\n            if self.optimizer is None or self.optimizer == \"GD\" or self.optimizer == \"\":\n\n                # classic gradient descend\n                with torch.no_grad():\n                    if update_phi_ab:\n                        self.phi_ab.add_(self.phi_ab.grad, alpha=-self.phi_ab_eta)\n                    if update_phi_inf:\n                        self.phi_inf.add_(self.phi_inf.grad, alpha=-self.phi_inf_eta)\n                # zero the gradients so they will not accumulate\n                if update_phi_ab:\n                    self.phi_ab.grad.zero_()\n                if update_phi_inf:\n                    self.phi_inf.grad.zero_()\n\n            # optimizer was specified\n            else:\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n\n        # return self.beta.detach(), self.b_inf.detach()\n        return {'phi_ab': self.phi_ab.detach(), 'phi_inf': self.phi_inf.detach()}\n\n    def get_variable_gradients(self, **kwargs):\n\n        grad_enable_dict = {\"phi_ab\": self.phi_ab.requires_grad,\n                            \"phi_inf\": self.phi_inf.requires_grad}\n\n        return grad_enable_dict\n\n    def set_variable_gradients(self, value=None, **kwargs):\n\n        if value is None:\n            raise ValueError(\"A value should be specified (True or False for general or dictionary)\")\n\n        if isinstance(value, dict):\n            self.phi_ab.requires_grad_(value[\"phi_ab\"])\n            self.phi_inf.requires_grad_(value[\"phi_inf\"])\n        else:\n            self.phi_ab.requires_grad_(value)\n            self.phi_inf.requires_grad_(value)\n\n    def get_variable_list(self, **kwargs):\n\n        return [self.phi_ab, self.phi_inf]\n\n\n# =============\n# Noise classes\n# =============\n\n\n__NOISE__ = {}\n\n\ndef register_noise(name: str):\n    def wrapper(cls):\n        if __NOISE__.get(name, None):\n            raise NameError(f\"Name {name} is already defined!\")\n        __NOISE__[name] = cls\n        return cls\n\n    return wrapper\n\n\ndef get_noise(name: str, **kwargs):\n    if __NOISE__.get(name, None) is None:\n        raise NameError(f\"Name {name} is not defined.\")\n    noiser = __NOISE__[name](**kwargs)\n    noiser.__name__ = name\n    return noiser\n\n\nclass Noise(ABC):\n    def __call__(self, data):\n        return self.forward(data)\n\n    @abstractmethod\n    def forward(self, data):\n        pass\n\n\n@register_noise(name='clean')\nclass Clean(Noise):\n    def forward(self, data):\n        return data\n\n\n@register_noise(name='gaussian')\nclass GaussianNoise(Noise):\n    def __init__(self, sigma):\n        self.sigma = sigma\n\n    def forward(self, data):\n        return data + torch.randn_like(data, device=data.device) * self.sigma\n\n\n@register_noise(name='poisson')\nclass PoissonNoise(Noise):\n    def __init__(self, rate):\n        self.rate = rate\n\n    def forward(self, data):\n        '''\n        Follow skimage.util.random_noise.\n        '''\n\n        # TODO: fix the addional Poission noise - osmosis_utils - adaption for debka\n\n        # version 3 (stack-overflow)\n\n        data = (data + 1.0) / 2.0\n        data = data.clamp(0, 1)\n        device = data.device\n        data = data.detach().cpu()\n        data = torch.from_numpy(np.random.poisson(data * 255.0 * self.rate) / 255.0 / self.rate)\n        data = data * 2.0 - 1.0\n        data = data.clamp(-1, 1)\n        return data.to(device)\n\n        # version 2 (skimage)\n        # if data.min() < 0:\n        #     low_clip = -1\n        # else:\n        #     low_clip = 0\n\n        # # Determine unique values in iamge & calculate the next power of two\n        # vals = torch.Tensor([len(torch.unique(data))])\n        # vals = 2 ** torch.ceil(torch.log2(vals))\n        # vals = vals.to(data.device)\n\n        # if low_clip == -1:\n        #     old_max = data.max()\n        #     data = (data + 1.0) / (old_max + 1.0)\n\n        # data = torch.poisson(data * vals) / float(vals)\n\n        # if low_clip == -1:\n        #     data = data * (old_max + 1.0) - 1.0\n\n        # return data.clamp(low_clip, 1.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:21:09.205742Z","iopub.execute_input":"2024-11-13T09:21:09.206136Z","iopub.status.idle":"2024-11-13T09:21:09.291788Z","shell.execute_reply.started":"2024-11-13T09:21:09.206097Z","shell.execute_reply":"2024-11-13T09:21:09.290714Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n# %% base functions for getting loss\n\n__LOSS__ = {}\n\n\ndef register_loss(name: str):\n    def wrapper(cls):\n        if __LOSS__.get(name, None):\n            raise NameError(f\"Name {name} is already registered!\")\n        __LOSS__[name] = cls\n        return cls\n\n    return wrapper\n\n\ndef get_loss(name: str, **kwargs):\n    if __LOSS__.get(name, None) is None:\n        raise NameError(f\"Name {name} is not defined.\")\n    return __LOSS__[name](**kwargs)\n\n\n# %% global exposure loss\n\n@register_loss(name='avrg_loss')\nclass Average_Loss(nn.Module):\n    \"\"\"\n    Global Exposure Control Loss\n    \"\"\"\n\n    def __init__(self):\n        super(Average_Loss, self).__init__()\n\n    def forward(self, x):\n        #  only color data (rgb) is required, depth is not required here - value should be [-1,1]\n\n        x_norm = x[:, 0:3, :, :]\n        mean = torch.mean(x_norm, dim=(2, 3))\n        avrg_loss = torch.sum(torch.abs(mean))\n\n        return avrg_loss\n\n\n# %% Value loss\n\n@register_loss(name='val_loss')\nclass Value_Loss(nn.Module):\n\n    def __init__(self, device=torch.device(\"cuda:0\"), **kwargs):\n        super(Value_Loss, self).__init__()\n        self.device = torch.device(device)\n\n    def forward(self, rgbd, **kwargs):\n        rgb = (rgbd[:, 0:3, :, :])\n        value = kwargs.get(\"value\", 0.7)\n        val_loss = (torch.maximum(rgb.abs() - value, torch.zeros_like(rgb)) ** 2).mean()\n\n        return val_loss\n\n\n# %% Auxiliary loss class which includes all the quality losses and their coefficients\n\nclass AuxiliaryLoss(nn.Module):\n    def __init__(self, losses_dictionary):\n        super(AuxiliaryLoss, self).__init__()\n\n        self.losses_dictionary = losses_dictionary\n        self.losses_list = [get_loss(key_ii) for key_ii in losses_dictionary.keys()]\n        self.loss_gammas = [torch.tensor(value_ii) for value_ii in losses_dictionary.values()]\n\n    def forward(self, x):\n        aux_loss = 0\n        aux_loss_dict = {}\n        # summing the losses according to their gammas\n        for gamma_ii, loss_ii, loss_name_ii in zip(self.loss_gammas, self.losses_list, self.losses_dictionary):\n            cur_loss = loss_ii.forward(x)\n            aux_loss += gamma_ii.to(x.device) * cur_loss\n            aux_loss_dict[loss_name_ii] = cur_loss.detach().cpu()\n        return aux_loss, aux_loss_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:22:11.391300Z","iopub.execute_input":"2024-11-13T09:22:11.391666Z","iopub.status.idle":"2024-11-13T09:22:11.409049Z","shell.execute_reply.started":"2024-11-13T09:22:11.391632Z","shell.execute_reply":"2024-11-13T09:22:11.408129Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from abc import ABC, abstractmethod\nimport torch\nimport numpy as np\n# import losses as losseso\n# import utils as utilso\nimport copy\n\n__CONDITIONING_METHOD__ = {}\n\n\ndef register_conditioning_method(name: str):\n    def wrapper(cls):\n        if __CONDITIONING_METHOD__.get(name, None):\n            raise NameError(f\"Name {name} is already registered!\")\n        __CONDITIONING_METHOD__[name] = cls\n        return cls\n\n    return wrapper\n\n\ndef get_conditioning_method(name: str, operator, noiser, **kwargs):\n    if __CONDITIONING_METHOD__.get(name, None) is None:\n        raise NameError(f\"Name {name} is not defined!\")\n    return __CONDITIONING_METHOD__[name](operator=operator, noiser=noiser, **kwargs)\n\n\nclass ConditioningMethod(ABC):\n    def __init__(self, operator, noiser, **kwargs):\n        self.operator = operator\n        self.noiser = noiser\n\n    def project(self, data, noisy_measurement, **kwargs):\n        return self.operator.project(data=data, measurement=noisy_measurement, **kwargs)\n\n    def grad_and_value(self, x_prev, x_0_hat, measurement, **kwargs):\n        if self.noiser.__name__ == 'gaussian':\n\n            difference = measurement - self.operator.forward(x_0_hat[:, 0:3], **kwargs)\n            loss = torch.linalg.norm(difference)\n            loss_grad = torch.autograd.grad(outputs=loss, inputs=x_prev)[0]\n            # loss_grad = torch.autograd.grad(outputs=loss, inputs=x_0_hat)[0]\n\n        elif self.noiser.__name__ == 'poisson':\n            Ax = self.operator.forward(x_0_hat, **kwargs)\n            difference = measurement - Ax\n            loss = torch.linalg.norm(difference) / measurement.abs()\n            loss = loss.mean()\n            loss_grad = torch.autograd.grad(outputs=loss, inputs=x_prev)[0]\n\n        else:\n            raise NotImplementedError\n\n        return loss_grad, loss\n\n    @abstractmethod\n    # def conditioning(self, x_t, measurement, noisy_measurement=None, **kwargs):\n    def conditioning(self, x_prev, x_t, x_0_hat, measurement, **kwargs):\n        pass\n\n\n@register_conditioning_method(name='osmosis')\nclass PosteriorSamplingOsmosis(ConditioningMethod):\n    def __init__(self, operator, noiser, **kwargs):\n        super().__init__(operator, noiser)\n        input_scale_str = kwargs.get('scale', 1.0)\n\n        # in case scale is single for all channels\n        try:\n            self.scale = torch.tensor([float(input_scale_str)])\n\n        # in case there is a scale value for each channel\n        except ValueError:\n            self.scale = torch.tensor([float(num_str.strip()) for num_str in input_scale_str.split(',')])\n\n        self.gradient_x_prev = kwargs.get('gradient_x_prev', False)\n\n        # sample pattern parameters\n        self.pattern_name = kwargs.get('pattern', 'original')\n        self.global_N = kwargs.get('global_N', 1)\n        self.local_M = kwargs.get('local_M', 1)\n        self.n_iter = kwargs.get('n_iter', 1)\n        self.update_start = kwargs.get('update_start', 1.0)\n\n        # Auxiliary loss information\n        aux_loss_dict = kwargs.get(\"aux_loss\", None)\n        if aux_loss_dict is not None:\n            aux_loss_dict = {key_ii: float(value_ii) for key_ii, value_ii in aux_loss_dict.items()}\n            # Quality loss object\n            self.aux_loss = AuxiliaryLoss(aux_loss_dict)\n        else:\n            self.aux_loss = None\n\n        # guiding loss function, loss weight (depth or none), is depth - what function and values\n        self.loss_function = kwargs.get(\"loss_function\", \"norm\")\n        self.loss_weight = kwargs.get(\"loss_weight\", None)\n        self.weight_function = kwargs.get(\"weight_function\", None)\n\n        # use gradient clipping (or image clipping)\n        gradient_clip_tmp = kwargs.get(\"gradient_clip\", \"False\")\n        gradient_clip_tmp = [num_str for num_str in gradient_clip_tmp.split(',')]\n        self.gradient_clip = str2bool(gradient_clip_tmp[0])\n\n        # if true - what values\n        if self.gradient_clip:\n            self.gradient_clip_value = float(gradient_clip_tmp[1].strip())\n        else:\n            self.gradient_clip_value = None\n\n    def grad_and_value(self, x_prev, x_0_hat, measurement, **kwargs):\n\n        # compute the degraded image on the unet prediction (operator) - in measurement file\n        degraded_image_tmp = self.operator.forward(x_0_hat, **kwargs)\n\n        # back to [-1,1]\n        degraded_image = 2 * degraded_image_tmp - 1\n\n        differance = (measurement - degraded_image)\n\n        # create the loss weights - multiply the differences\n        loss_weight = set_loss_weight(loss_weight_type=self.loss_weight,\n                                             weight_function=self.weight_function,\n                                             degraded_image=degraded_image_tmp.detach(),\n                                             x_0_hat=x_0_hat.detach())\n        differance = differance * loss_weight\n\n        # loss function - norm2\n        if self.loss_function == 'norm':\n            loss = torch.linalg.norm(differance)\n            # calculated for visualization\n            sep_loss = torch.norm(differance.detach().cpu(), p=2, dim=[1, 2, 3]).numpy()\n\n        # Mean square error\n        elif self.loss_function == \"mse\":\n            mse = differance ** 2\n            mse = mse.mean(dim=(1, 2, 3))\n            loss = mse.sum()\n            # calculated for visualization\n            sep_loss = mse.detach().cpu().numpy()\n\n        # No other loss\n        else:\n            raise NotImplementedError\n\n        return sep_loss, loss, degraded_image_tmp.detach()\n\n    def conditioning(self, x_prev, x_t, x_0_hat, measurement, **kwargs):\n\n        freeze_phi = kwargs.get(\"freeze_phi\", False)\n        time_index = kwargs.get(\"time_index\", None)\n\n        # when the gradient is w.r.t x0, the x_prev gradients and history of the x0 prediction are not required\n        if not self.gradient_x_prev:\n            x_0_hat = x_0_hat.detach().to(x_0_hat.device)\n            x_0_hat.requires_grad_(True)\n            x_0_hat = x_0_hat.to(x_0_hat.device)\n            x_prev.requires_grad_(False)\n\n        # calculate the losses\n        with torch.set_grad_enabled(True):\n\n            # phi's require gradients when we update them, hence when freeze_phi is False\n            self.operator.set_variable_gradients(value=not freeze_phi)\n\n            # the number of inner optimization num of steps should be 1 if freezing phi,\n            # since there is no optimizing at all in this case\n            inner_optimize_length = 1 if freeze_phi else self.n_iter\n\n            for optimize_ii in range(inner_optimize_length):\n\n                # compute the loss after applying the operator, sep_loss is relevant for multiple images\n                sep_loss, loss, degraded_image_01 = self.grad_and_value(x_prev=x_prev,\n                                                                        x_0_hat=x_0_hat,\n                                                                        measurement=measurement,\n                                                                        time_index=time_index)\n\n                # total loss refers to the original loss or to the loss of the x\n                if self.aux_loss is not None:\n                    aux_loss, aux_loss_dict = self.aux_loss.forward(x_0_hat)\n                    total_loss = loss + aux_loss\n                else:\n                    aux_loss_dict = None\n                    total_loss = loss\n\n                # calculate the backward graph\n                if optimize_ii == (inner_optimize_length - 1):\n                    if freeze_phi:\n                        # calculate graph w.r.t x_prev\n                        total_loss.backward(inputs=[x_prev])\n                    else:\n                        # calculate graph w.r.t x_prev and phi's\n                        total_loss.backward(inputs=[x_prev] + self.operator.get_variable_list())\n                else:\n                    # when optimize only the phi's, we specify it for faster run time\n                    total_loss.backward(inputs=self.operator.get_variable_list())\n\n                # optimize phi's, in case of freeze phi true - optimization is not done\n                variables_dict = self.operator.optimize(freeze_phi=freeze_phi)\n\n            # update x_t\n            with torch.no_grad():\n\n                # remove - opher\n                # # update guidance scale\n                # scale_norm = utilso.set_guidance_scale_norm(norm_type=self.scale_norm, x_0_hat=x_0_hat.detach(),\n                #                                             x_t=x_t.detach(), x_prev=x_prev,\n                #                                             sample_added_noise=sample_added_noise)\n                # # reshape the scale according to [b,c,h,w]\n                # guidance_scale = scale_norm * self.scale[None, ..., None, None].to(x_prev.device)\n\n                # reshape the scale according to [b,c,h,w]\n                guidance_scale = self.scale[None, ..., None, None].to(x_prev.device)\n\n                # update x_t - gradient w.r.t x_t\n                if self.gradient_x_prev:\n\n                    if self.gradient_clip:\n                        grads = torch.clamp(x_prev.grad,\n                                            min=-self.gradient_clip_value,\n                                            max=self.gradient_clip_value)\n                    else:\n                        grads = x_prev.grad\n\n                    x_t -= guidance_scale * grads\n                    gradients = x_prev.grad.cpu()\n\n                # update x_t - gradient w.r.t x_0_pred\n                else:\n                    x_t -= guidance_scale * x_0_hat.grad\n                    gradients = x_0_hat.grad.cpu()\n\n        return x_t, sep_loss, variables_dict, gradients, aux_loss_dict\n\n\n@register_conditioning_method(name='ps')\nclass PosteriorSampling(ConditioningMethod):\n    def __init__(self, operator, noiser, **kwargs):\n        super().__init__(operator, noiser)\n\n        input_scale_str = kwargs.get('scale', 1.0)\n        # in case scale is single for all channels\n        try:\n            self.scale = torch.tensor([float(input_scale_str)])\n        # in case there is a scale value for each channel\n        except ValueError:\n            self.scale = torch.tensor([float(num_str.strip()) for num_str in input_scale_str.split(',')])\n\n    def conditioning(self, x_prev, x_t, x_0_hat, measurement, **kwargs):\n        norm_grad, norm = self.grad_and_value(x_prev=x_prev, x_0_hat=x_0_hat, measurement=measurement, **kwargs)\n        x_t -= norm_grad * self.scale[None, ..., None, None].to(x_prev.device)\n\n        return x_t, norm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:23:27.206840Z","iopub.execute_input":"2024-11-13T09:23:27.207329Z","iopub.status.idle":"2024-11-13T09:23:27.269254Z","shell.execute_reply.started":"2024-11-13T09:23:27.207280Z","shell.execute_reply":"2024-11-13T09:23:27.268257Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"\"\"\"\nLogger copied from OpenAI baselines to avoid extra RL-based dependencies:\nhttps://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/logger.py\n\"\"\"\n\nimport os\nimport sys\nimport shutil\nimport os.path as osp\nimport json\nimport time\nimport datetime\nimport tempfile\nimport warnings\nfrom collections import defaultdict\nfrom contextlib import contextmanager\n\nDEBUG = 10\nINFO = 20\nWARN = 30\nERROR = 40\n\nDISABLED = 50\n\n\nclass KVWriter(object):\n    def writekvs(self, kvs):\n        raise NotImplementedError\n\n\nclass SeqWriter(object):\n    def writeseq(self, seq):\n        raise NotImplementedError\n\n\nclass HumanOutputFormat(KVWriter, SeqWriter):\n    def __init__(self, filename_or_file):\n        if isinstance(filename_or_file, str):\n            self.file = open(filename_or_file, \"wt\")\n            self.own_file = True\n        else:\n            assert hasattr(filename_or_file, \"read\"), (\n                    \"expected file or str, got %s\" % filename_or_file\n            )\n            self.file = filename_or_file\n            self.own_file = False\n\n    def writekvs(self, kvs):\n        # Create strings for printing\n        key2str = {}\n        for (key, val) in sorted(kvs.items()):\n            if hasattr(val, \"__float__\"):\n                valstr = \"%-8.3g\" % val\n            else:\n                valstr = str(val)\n            key2str[self._truncate(key)] = self._truncate(valstr)\n\n        # Find max widths\n        if len(key2str) == 0:\n            print(\"WARNING: tried to write empty key-value dict\")\n            return\n        else:\n            keywidth = max(map(len, key2str.keys()))\n            valwidth = max(map(len, key2str.values()))\n\n        # Write out the data\n        dashes = \"-\" * (keywidth + valwidth + 7)\n        lines = [dashes]\n        for (key, val) in sorted(key2str.items(), key=lambda kv: kv[0].lower()):\n            lines.append(\n                \"| %s%s | %s%s |\"\n                % (key, \" \" * (keywidth - len(key)), val, \" \" * (valwidth - len(val)))\n            )\n        lines.append(dashes)\n        self.file.write(\"\\n\".join(lines) + \"\\n\")\n\n        # Flush the output to the file\n        self.file.flush()\n\n    def _truncate(self, s):\n        maxlen = 30\n        return s[: maxlen - 3] + \"...\" if len(s) > maxlen else s\n\n    def writeseq(self, seq):\n        seq = list(seq)\n        for (i, elem) in enumerate(seq):\n            self.file.write(elem)\n            if i < len(seq) - 1:  # add space unless this is the last one\n                self.file.write(\" \")\n        self.file.write(\"\\n\")\n        self.file.flush()\n\n    def close(self):\n        if self.own_file:\n            self.file.close()\n\n\nclass JSONOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \"wt\")\n\n    def writekvs(self, kvs):\n        for k, v in sorted(kvs.items()):\n            if hasattr(v, \"dtype\"):\n                kvs[k] = float(v)\n        self.file.write(json.dumps(kvs) + \"\\n\")\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\n\nclass CSVOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \"w+t\")\n        self.keys = []\n        self.sep = \",\"\n\n    def writekvs(self, kvs):\n        # Add our current row to the history\n        extra_keys = list(kvs.keys() - self.keys)\n        extra_keys.sort()\n        if extra_keys:\n            self.keys.extend(extra_keys)\n            self.file.seek(0)\n            lines = self.file.readlines()\n            self.file.seek(0)\n            for (i, k) in enumerate(self.keys):\n                if i > 0:\n                    self.file.write(\",\")\n                self.file.write(k)\n            self.file.write(\"\\n\")\n            for line in lines[1:]:\n                self.file.write(line[:-1])\n                self.file.write(self.sep * len(extra_keys))\n                self.file.write(\"\\n\")\n        for (i, k) in enumerate(self.keys):\n            if i > 0:\n                self.file.write(\",\")\n            v = kvs.get(k)\n            if v is not None:\n                self.file.write(str(v))\n        self.file.write(\"\\n\")\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\n\nclass TensorBoardOutputFormat(KVWriter):\n    \"\"\"\n    Dumps key/value pairs into TensorBoard's numeric format.\n    \"\"\"\n\n    def __init__(self, dir):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \"events\"\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python.util import compat\n\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs):\n        def summary_val(k, v):\n            kwargs = {\"tag\": k, \"simple_value\": float(v)}\n            return self.tf.Summary.Value(**kwargs)\n\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = (\n            self.step\n        )  # is there any reason why you'd want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self):\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n\n\ndef make_output_format(format, ev_dir, log_suffix=\"\"):\n    os.makedirs(ev_dir, exist_ok=True)\n    if format == \"stdout\":\n        return HumanOutputFormat(sys.stdout)\n    elif format == \"log\":\n        return HumanOutputFormat(osp.join(ev_dir, \"log%s.txt\" % log_suffix))\n    elif format == \"json\":\n        return JSONOutputFormat(osp.join(ev_dir, \"progress%s.json\" % log_suffix))\n    elif format == \"csv\":\n        return CSVOutputFormat(osp.join(ev_dir, \"progress%s.csv\" % log_suffix))\n    elif format == \"tensorboard\":\n        return TensorBoardOutputFormat(osp.join(ev_dir, \"tb%s\" % log_suffix))\n    else:\n        raise ValueError(\"Unknown format specified: %s\" % (format,))\n\n\n# ================================================================\n# API\n# ================================================================\n\n\ndef logkv(key, val):\n    \"\"\"\n    Log a value of some diagnostic\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    \"\"\"\n    get_current().logkv(key, val)\n\n\ndef logkv_mean(key, val):\n    \"\"\"\n    The same as logkv(), but if called many times, values averaged.\n    \"\"\"\n    get_current().logkv_mean(key, val)\n\n\ndef logkvs(d):\n    \"\"\"\n    Log a dictionary of key-value pairs\n    \"\"\"\n    for (k, v) in d.items():\n        logkv(k, v)\n\n\ndef dumpkvs():\n    \"\"\"\n    Write all of the diagnostics from the current iteration\n    \"\"\"\n    return get_current().dumpkvs()\n\n\ndef getkvs():\n    return get_current().name2val\n\n\ndef log(*args, level=INFO):\n    \"\"\"\n    Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).\n    \"\"\"\n    get_current().log(*args, level=level)\n\n\ndef debug(*args):\n    log(*args, level=DEBUG)\n\n\ndef info(*args):\n    log(*args, level=INFO)\n\n\ndef warn(*args):\n    log(*args, level=WARN)\n\n\ndef error(*args):\n    log(*args, level=ERROR)\n\n\ndef set_level(level):\n    \"\"\"\n    Set logging threshold on current logger.\n    \"\"\"\n    get_current().set_level(level)\n\n\ndef set_comm(comm):\n    get_current().set_comm(comm)\n\n\ndef get_dir():\n    \"\"\"\n    Get directory that log files are being written to.\n    will be None if there is no output directory (i.e., if you didn't call start)\n    \"\"\"\n    return get_current().get_dir()\n\n\nrecord_tabular = logkv\ndump_tabular = dumpkvs\n\n\n@contextmanager\ndef profile_kv(scopename):\n    logkey = \"wait_\" + scopename\n    tstart = time.time()\n    try:\n        yield\n    finally:\n        get_current().name2val[logkey] += time.time() - tstart\n\n\ndef profile(n):\n    \"\"\"\n    Usage:\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"\n\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return decorator_with_name\n\n\n# ================================================================\n# Backend\n# ================================================================\n\n\ndef get_current():\n    if Logger.CURRENT is None:\n        _configure_default_logger()\n\n    return Logger.CURRENT\n\n\nclass Logger(object):\n    DEFAULT = None  # A logger with no output files. (See right below class definition)\n    # So that you can still log to the terminal without setting up any output files\n    CURRENT = None  # Current logger being used by the free functions above\n\n    def __init__(self, dir, output_formats, comm=None):\n        self.name2val = defaultdict(float)  # values this iteration\n        self.name2cnt = defaultdict(int)\n        self.level = INFO\n        self.dir = dir\n        self.output_formats = output_formats\n        self.comm = comm\n\n    # Logging API, forwarded\n    # ----------------------------------------\n    def logkv(self, key, val):\n        self.name2val[key] = val\n\n    def logkv_mean(self, key, val):\n        oldval, cnt = self.name2val[key], self.name2cnt[key]\n        self.name2val[key] = oldval * cnt / (cnt + 1) + val / (cnt + 1)\n        self.name2cnt[key] = cnt + 1\n\n    def dumpkvs(self):\n        if self.comm is None:\n            d = self.name2val\n        else:\n            d = mpi_weighted_mean(\n                self.comm,\n                {\n                    name: (val, self.name2cnt.get(name, 1))\n                    for (name, val) in self.name2val.items()\n                },\n            )\n            if self.comm.rank != 0:\n                d[\"dummy\"] = 1  # so we don't get a warning about empty dict\n        out = d.copy()  # Return the dict for unit testing purposes\n        for fmt in self.output_formats:\n            if isinstance(fmt, KVWriter):\n                fmt.writekvs(d)\n        self.name2val.clear()\n        self.name2cnt.clear()\n        return out\n\n    def log(self, *args, level=INFO):\n        if self.level <= level:\n            self._do_log(args)\n\n    # Configuration\n    # ----------------------------------------\n    def set_level(self, level):\n        self.level = level\n\n    def set_comm(self, comm):\n        self.comm = comm\n\n    def get_dir(self):\n        return self.dir\n\n    def close(self):\n        for fmt in self.output_formats:\n            fmt.close()\n\n    # Misc\n    # ----------------------------------------\n    def _do_log(self, args):\n        for fmt in self.output_formats:\n            if isinstance(fmt, SeqWriter):\n                fmt.writeseq(map(str, args))\n\n\ndef get_rank_without_mpi_import():\n    # check environment variables here instead of importing mpi4py\n    # to avoid calling MPI_Init() when this module is imported\n    for varname in [\"PMI_RANK\", \"OMPI_COMM_WORLD_RANK\"]:\n        if varname in os.environ:\n            return int(os.environ[varname])\n    return 0\n\n\ndef mpi_weighted_mean(comm, local_name2valcount):\n    \"\"\"\n    Copied from: https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -> (value, count)\n    Returns: key -> mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum = defaultdict(float)\n        name2count = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for (name, (val, count)) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\n                            \"WARNING: tried to compute mean on non-float {}={}\".format(\n                                name, val\n                            )\n                        )\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    else:\n        return {}\n\n\ndef configure(dir=None, format_strs=None, comm=None, log_suffix=\"\"):\n    \"\"\"\n    If comm is provided, average all numerical stats across that comm\n    \"\"\"\n    if dir is None:\n        dir = os.getenv(\"OPENAI_LOGDIR\")\n    if dir is None:\n        dir = osp.join(\n            tempfile.gettempdir(),\n            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"),\n        )\n    assert isinstance(dir, str)\n    dir = os.path.expanduser(dir)\n    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n\n    rank = get_rank_without_mpi_import()\n    if rank > 0:\n        log_suffix = log_suffix + \"-rank%03i\" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            # format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log,csv\").split(\",\")\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log\").split(\",\")\n        else:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT_MPI\", \"log\").split(\",\")\n    format_strs = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)\n    if output_formats:\n        log(\"Logging to %s\" % dir)\n\n\ndef _configure_default_logger():\n    configure()\n    Logger.DEFAULT = Logger.CURRENT\n\n\ndef reset():\n    if Logger.CURRENT is not Logger.DEFAULT:\n        Logger.CURRENT.close()\n        Logger.CURRENT = Logger.DEFAULT\n        log(\"Reset logger\")\n\n\n@contextmanager\ndef scoped_configure(dir=None, format_strs=None, comm=None):\n    prevlogger = Logger.CURRENT\n    configure(dir=dir, format_strs=format_strs, comm=comm)\n    try:\n        yield\n    finally:\n        Logger.CURRENT.close()\n        Logger.CURRENT = prevlogger","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:25:06.252559Z","iopub.execute_input":"2024-11-13T09:25:06.252911Z","iopub.status.idle":"2024-11-13T09:25:06.322005Z","shell.execute_reply.started":"2024-11-13T09:25:06.252879Z","shell.execute_reply":"2024-11-13T09:25:06.321065Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import cv2\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nimport glob\nfrom PIL import Image\nfrom natsort import natsorted\n\nimport torch\nfrom torch.utils.data import Dataset\n\n\n# %% ImageFolder Dataset\n\nclass ImagesFolder(Dataset):\n\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.images_list = natsorted(os.listdir(root_dir))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_list)\n\n    def __getitem__(self, idx):\n        try:\n            image = Image.open(os.path.join(self.root_dir, self.images_list[idx]))\n        except:\n            print(\"\\n**************\\nexpect\\n**************\\n\")\n            image = cv2.imread(os.path.join(self.root_dir, self.images_list[idx]), cv2.IMREAD_UNCHANGED)\n            image = image // 255\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image, self.images_list[idx]\n\n\n# %% ImageFolder Dataset with gt (simulation)\n\nclass ImagesFolder_GT_results(Dataset):\n\n    def __init__(self, gt_dir, results_dir, transform=None):\n        self.gt_dir = gt_dir\n        self.results_dir = results_dir\n\n        self.gt_list = natsorted(glob.glob(pjoin(gt_dir, \"*.*\")))\n        self.simulate_list = natsorted(glob.glob(pjoin(results_dir, \"*ref.png\")))\n        self.rgb_list = natsorted(glob.glob(pjoin(results_dir, \"*rgb.png\")))\n        self.depth_list = natsorted(glob.glob(pjoin(results_dir, \"*depth.png\")))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.gt_list)\n\n    def __getitem__(self, idx):\n        image_name = os.path.splitext(os.path.basename(self.gt_list[idx]))[0]\n        gt = Image.open(self.gt_list[idx])\n        simulate = Image.open(self.simulate_list[idx])\n        rgb = Image.open(self.rgb_list[idx])\n        depth = Image.open(self.depth_list[idx])\n\n        if self.transform is not None:\n            gt = self.transform(gt)\n            simulate = self.transform(simulate)\n            rgb = self.transform(rgb)\n            depth = self.transform(depth)\n\n        return gt, simulate, rgb, depth, image_name\n\n\n# %% ImageFolder Dataset with gt\nclass ImagesFolder_GT(Dataset):\n\n    def __init__(self, root_dir, gt_rgb_dir, gt_depth_dir, transform=None):\n        self.gt_rgb_dir = gt_rgb_dir\n        self.gt_depth_dir = gt_depth_dir\n        self.root_dir = root_dir\n\n        self.gt_rgb_list = natsorted(glob.glob(pjoin(gt_rgb_dir, \"*.*\")))\n        self.gt_depth_list = natsorted(glob.glob(pjoin(gt_depth_dir, \"*.*\")))\n        self.images_list = natsorted(glob.glob(pjoin(root_dir, \"*.*\")))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.gt_rgb_list)\n\n    def __getitem__(self, idx):\n        image_name = os.path.basename(self.images_list[idx])\n        image = Image.open(self.images_list[idx])\n        gt_rgb_image = Image.open(self.gt_rgb_list[idx])\n\n        gt_depth_image_tmp = cv2.imread(self.gt_depth_list[idx], cv2.IMREAD_UNCHANGED)\n        if gt_depth_image_tmp.dtype == 'uint16':\n            gt_depth_image = Image.fromarray((gt_depth_image_tmp//256).astype(np.uint8))\n        else:\n            gt_depth_image = Image.fromarray(gt_depth_image_tmp)\n            # gt_depth_image = Image.open(self.gt_depth_list[idx])\n\n        if self.transform is not None:\n            image = self.transform(image)\n            gt_rgb_image = self.transform(gt_rgb_image)\n\n            # it is a single channel image (only depth), so preprocess is required\n            # gt_depth_image = Image.merge(\"RGB\", (gt_depth_image,gt_depth_image,gt_depth_image))\n            gt_depth_image = gt_depth_image.convert(mode=\"RGB\")\n            gt_depth_image = self.transform(gt_depth_image)\n\n        return [image, gt_rgb_image, gt_depth_image], image_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:50:39.973847Z","iopub.execute_input":"2024-11-13T09:50:39.974188Z","iopub.status.idle":"2024-11-13T09:50:40.026631Z","shell.execute_reply.started":"2024-11-13T09:50:39.974155Z","shell.execute_reply":"2024-11-13T09:50:40.025857Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"\"\"\"\nLogger copied from OpenAI baselines to avoid extra RL-based dependencies:\nhttps://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/logger.py\n\"\"\"\n\nimport os\nimport sys\nimport shutil\nimport os.path as osp\nimport json\nimport time\nimport datetime\nimport tempfile\nimport warnings\nfrom collections import defaultdict\nfrom contextlib import contextmanager\n\nDEBUG = 10\nINFO = 20\nWARN = 30\nERROR = 40\n\nDISABLED = 50\n\n\nclass KVWriter(object):\n    def writekvs(self, kvs):\n        raise NotImplementedError\n\n\nclass SeqWriter(object):\n    def writeseq(self, seq):\n        raise NotImplementedError\n\n\nclass HumanOutputFormat(KVWriter, SeqWriter):\n    def __init__(self, filename_or_file):\n        if isinstance(filename_or_file, str):\n            self.file = open(filename_or_file, \"wt\")\n            self.own_file = True\n        else:\n            assert hasattr(filename_or_file, \"read\"), (\n                    \"expected file or str, got %s\" % filename_or_file\n            )\n            self.file = filename_or_file\n            self.own_file = False\n\n    def writekvs(self, kvs):\n        # Create strings for printing\n        key2str = {}\n        for (key, val) in sorted(kvs.items()):\n            if hasattr(val, \"__float__\"):\n                valstr = \"%-8.3g\" % val\n            else:\n                valstr = str(val)\n            key2str[self._truncate(key)] = self._truncate(valstr)\n\n        # Find max widths\n        if len(key2str) == 0:\n            print(\"WARNING: tried to write empty key-value dict\")\n            return\n        else:\n            keywidth = max(map(len, key2str.keys()))\n            valwidth = max(map(len, key2str.values()))\n\n        # Write out the data\n        dashes = \"-\" * (keywidth + valwidth + 7)\n        lines = [dashes]\n        for (key, val) in sorted(key2str.items(), key=lambda kv: kv[0].lower()):\n            lines.append(\n                \"| %s%s | %s%s |\"\n                % (key, \" \" * (keywidth - len(key)), val, \" \" * (valwidth - len(val)))\n            )\n        lines.append(dashes)\n        self.file.write(\"\\n\".join(lines) + \"\\n\")\n\n        # Flush the output to the file\n        self.file.flush()\n\n    def _truncate(self, s):\n        maxlen = 30\n        return s[: maxlen - 3] + \"...\" if len(s) > maxlen else s\n\n    def writeseq(self, seq):\n        seq = list(seq)\n        for (i, elem) in enumerate(seq):\n            self.file.write(elem)\n            if i < len(seq) - 1:  # add space unless this is the last one\n                self.file.write(\" \")\n        self.file.write(\"\\n\")\n        self.file.flush()\n\n    def close(self):\n        if self.own_file:\n            self.file.close()\n\n\nclass JSONOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \"wt\")\n\n    def writekvs(self, kvs):\n        for k, v in sorted(kvs.items()):\n            if hasattr(v, \"dtype\"):\n                kvs[k] = float(v)\n        self.file.write(json.dumps(kvs) + \"\\n\")\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\n\nclass CSVOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \"w+t\")\n        self.keys = []\n        self.sep = \",\"\n\n    def writekvs(self, kvs):\n        # Add our current row to the history\n        extra_keys = list(kvs.keys() - self.keys)\n        extra_keys.sort()\n        if extra_keys:\n            self.keys.extend(extra_keys)\n            self.file.seek(0)\n            lines = self.file.readlines()\n            self.file.seek(0)\n            for (i, k) in enumerate(self.keys):\n                if i > 0:\n                    self.file.write(\",\")\n                self.file.write(k)\n            self.file.write(\"\\n\")\n            for line in lines[1:]:\n                self.file.write(line[:-1])\n                self.file.write(self.sep * len(extra_keys))\n                self.file.write(\"\\n\")\n        for (i, k) in enumerate(self.keys):\n            if i > 0:\n                self.file.write(\",\")\n            v = kvs.get(k)\n            if v is not None:\n                self.file.write(str(v))\n        self.file.write(\"\\n\")\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\n\nclass TensorBoardOutputFormat(KVWriter):\n    \"\"\"\n    Dumps key/value pairs into TensorBoard's numeric format.\n    \"\"\"\n\n    def __init__(self, dir):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \"events\"\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python.util import compat\n\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs):\n        def summary_val(k, v):\n            kwargs = {\"tag\": k, \"simple_value\": float(v)}\n            return self.tf.Summary.Value(**kwargs)\n\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = (\n            self.step\n        )  # is there any reason why you'd want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self):\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n\n\ndef make_output_format(format, ev_dir, log_suffix=\"\"):\n    os.makedirs(ev_dir, exist_ok=True)\n    if format == \"stdout\":\n        return HumanOutputFormat(sys.stdout)\n    elif format == \"log\":\n        return HumanOutputFormat(osp.join(ev_dir, \"log%s.txt\" % log_suffix))\n    elif format == \"json\":\n        return JSONOutputFormat(osp.join(ev_dir, \"progress%s.json\" % log_suffix))\n    elif format == \"csv\":\n        return CSVOutputFormat(osp.join(ev_dir, \"progress%s.csv\" % log_suffix))\n    elif format == \"tensorboard\":\n        return TensorBoardOutputFormat(osp.join(ev_dir, \"tb%s\" % log_suffix))\n    else:\n        raise ValueError(\"Unknown format specified: %s\" % (format,))\n\n\n# ================================================================\n# API\n# ================================================================\n\n\ndef logkv(key, val):\n    \"\"\"\n    Log a value of some diagnostic\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    \"\"\"\n    get_current().logkv(key, val)\n\n\ndef logkv_mean(key, val):\n    \"\"\"\n    The same as logkv(), but if called many times, values averaged.\n    \"\"\"\n    get_current().logkv_mean(key, val)\n\n\ndef logkvs(d):\n    \"\"\"\n    Log a dictionary of key-value pairs\n    \"\"\"\n    for (k, v) in d.items():\n        logkv(k, v)\n\n\ndef dumpkvs():\n    \"\"\"\n    Write all of the diagnostics from the current iteration\n    \"\"\"\n    return get_current().dumpkvs()\n\n\ndef getkvs():\n    return get_current().name2val\n\n\ndef log(*args, level=INFO):\n    \"\"\"\n    Write the sequence of args, with no separators, to the console and output files (if you've configured an output file).\n    \"\"\"\n    get_current().log(*args, level=level)\n\n\ndef debug(*args):\n    log(*args, level=DEBUG)\n\n\ndef info(*args):\n    log(*args, level=INFO)\n\n\ndef warn(*args):\n    log(*args, level=WARN)\n\n\ndef error(*args):\n    log(*args, level=ERROR)\n\n\ndef set_level(level):\n    \"\"\"\n    Set logging threshold on current logger.\n    \"\"\"\n    get_current().set_level(level)\n\n\ndef set_comm(comm):\n    get_current().set_comm(comm)\n\n\ndef get_dir():\n    \"\"\"\n    Get directory that log files are being written to.\n    will be None if there is no output directory (i.e., if you didn't call start)\n    \"\"\"\n    return get_current().get_dir()\n\n\nrecord_tabular = logkv\ndump_tabular = dumpkvs\n\n\n@contextmanager\ndef profile_kv(scopename):\n    logkey = \"wait_\" + scopename\n    tstart = time.time()\n    try:\n        yield\n    finally:\n        get_current().name2val[logkey] += time.time() - tstart\n\n\ndef profile(n):\n    \"\"\"\n    Usage:\n    @profile(\"my_func\")\n    def my_func(): code\n    \"\"\"\n\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with profile_kv(n):\n                return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return decorator_with_name\n\n\n# ================================================================\n# Backend\n# ================================================================\n\n\ndef get_current():\n    if Logger.CURRENT is None:\n        _configure_default_logger()\n\n    return Logger.CURRENT\n\n\nclass Logger(object):\n    DEFAULT = None  # A logger with no output files. (See right below class definition)\n    # So that you can still log to the terminal without setting up any output files\n    CURRENT = None  # Current logger being used by the free functions above\n\n    def __init__(self, dir, output_formats, comm=None):\n        self.name2val = defaultdict(float)  # values this iteration\n        self.name2cnt = defaultdict(int)\n        self.level = INFO\n        self.dir = dir\n        self.output_formats = output_formats\n        self.comm = comm\n\n    # Logging API, forwarded\n    # ----------------------------------------\n    def logkv(self, key, val):\n        self.name2val[key] = val\n\n    def logkv_mean(self, key, val):\n        oldval, cnt = self.name2val[key], self.name2cnt[key]\n        self.name2val[key] = oldval * cnt / (cnt + 1) + val / (cnt + 1)\n        self.name2cnt[key] = cnt + 1\n\n    def dumpkvs(self):\n        if self.comm is None:\n            d = self.name2val\n        else:\n            d = mpi_weighted_mean(\n                self.comm,\n                {\n                    name: (val, self.name2cnt.get(name, 1))\n                    for (name, val) in self.name2val.items()\n                },\n            )\n            if self.comm.rank != 0:\n                d[\"dummy\"] = 1  # so we don't get a warning about empty dict\n        out = d.copy()  # Return the dict for unit testing purposes\n        for fmt in self.output_formats:\n            if isinstance(fmt, KVWriter):\n                fmt.writekvs(d)\n        self.name2val.clear()\n        self.name2cnt.clear()\n        return out\n\n    def log(self, *args, level=INFO):\n        if self.level <= level:\n            self._do_log(args)\n\n    # Configuration\n    # ----------------------------------------\n    def set_level(self, level):\n        self.level = level\n\n    def set_comm(self, comm):\n        self.comm = comm\n\n    def get_dir(self):\n        return self.dir\n\n    def close(self):\n        for fmt in self.output_formats:\n            fmt.close()\n\n    # Misc\n    # ----------------------------------------\n    def _do_log(self, args):\n        for fmt in self.output_formats:\n            if isinstance(fmt, SeqWriter):\n                fmt.writeseq(map(str, args))\n\n\ndef get_rank_without_mpi_import():\n    # check environment variables here instead of importing mpi4py\n    # to avoid calling MPI_Init() when this module is imported\n    for varname in [\"PMI_RANK\", \"OMPI_COMM_WORLD_RANK\"]:\n        if varname in os.environ:\n            return int(os.environ[varname])\n    return 0\n\n\ndef mpi_weighted_mean(comm, local_name2valcount):\n    \"\"\"\n    Copied from: https://github.com/openai/baselines/blob/ea25b9e8b234e6ee1bca43083f8f3cf974143998/baselines/common/mpi_util.py#L110\n    Perform a weighted average over dicts that are each on a different node\n    Input: local_name2valcount: dict mapping key -> (value, count)\n    Returns: key -> mean\n    \"\"\"\n    all_name2valcount = comm.gather(local_name2valcount)\n    if comm.rank == 0:\n        name2sum = defaultdict(float)\n        name2count = defaultdict(float)\n        for n2vc in all_name2valcount:\n            for (name, (val, count)) in n2vc.items():\n                try:\n                    val = float(val)\n                except ValueError:\n                    if comm.rank == 0:\n                        warnings.warn(\n                            \"WARNING: tried to compute mean on non-float {}={}\".format(\n                                name, val\n                            )\n                        )\n                else:\n                    name2sum[name] += val * count\n                    name2count[name] += count\n        return {name: name2sum[name] / name2count[name] for name in name2sum}\n    else:\n        return {}\n\n\ndef configure(dir=None, format_strs=None, comm=None, log_suffix=\"\"):\n    \"\"\"\n    If comm is provided, average all numerical stats across that comm\n    \"\"\"\n    if dir is None:\n        dir = os.getenv(\"OPENAI_LOGDIR\")\n    if dir is None:\n        dir = osp.join(\n            tempfile.gettempdir(),\n            datetime.datetime.now().strftime(\"openai-%Y-%m-%d-%H-%M-%S-%f\"),\n        )\n    assert isinstance(dir, str)\n    dir = os.path.expanduser(dir)\n    os.makedirs(os.path.expanduser(dir), exist_ok=True)\n\n    rank = get_rank_without_mpi_import()\n    if rank > 0:\n        log_suffix = log_suffix + \"-rank%03i\" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            # format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log,csv\").split(\",\")\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT\", \"stdout,log\").split(\",\")\n        else:\n            format_strs = os.getenv(\"OPENAI_LOG_FORMAT_MPI\", \"log\").split(\",\")\n    format_strs = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats, comm=comm)\n    if output_formats:\n        log(\"Logging to %s\" % dir)\n\n\ndef _configure_default_logger():\n    configure()\n    Logger.DEFAULT = Logger.CURRENT\n\n\ndef reset():\n    if Logger.CURRENT is not Logger.DEFAULT:\n        Logger.CURRENT.close()\n        Logger.CURRENT = Logger.DEFAULT\n        log(\"Reset logger\")\n\n\n@contextmanager\ndef scoped_configure(dir=None, format_strs=None, comm=None):\n    prevlogger = Logger.CURRENT\n    configure(dir=dir, format_strs=format_strs, comm=comm)\n    try:\n        yield\n    finally:\n        Logger.CURRENT.close()\n        Logger.CURRENT = prevlogger","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:51:38.171893Z","iopub.execute_input":"2024-11-13T09:51:38.172247Z","iopub.status.idle":"2024-11-13T09:51:38.241023Z","shell.execute_reply.started":"2024-11-13T09:51:38.172198Z","shell.execute_reply":"2024-11-13T09:51:38.239928Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"import torch.nn as nn\nimport torch as th\nimport math\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = th.exp(\n        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n    ).to(device=timesteps.device)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:58:26.628642Z","iopub.execute_input":"2024-11-13T09:58:26.629536Z","iopub.status.idle":"2024-11-13T09:58:26.640151Z","shell.execute_reply.started":"2024-11-13T09:58:26.629481Z","shell.execute_reply":"2024-11-13T09:58:26.639023Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"import sys\n\nimport numpy as np\nfrom functools import partial\nimport os\nfrom os.path import join as pjoin\nfrom argparse import ArgumentParser\nfrom PIL import Image\nimport datetime\n\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as tvtf\nfrom torchvision.utils import make_grid\n\n# from noise import get_noise, get_operator\n# from condition import get_conditioning_method   \n# from unet import create_model\n# from gaussian_diffusion import create_sampler\nimport logger\n# import utils as utilso\n# import data as datao\n\n\ndef main():\n    args = arguments_from_file(CONFIG_FILE)\n    args.image_size = args.unet_model['image_size']\n    args.unet_model['model_path'] = os.path.abspath(\"/kaggle/input/image-enhancer-rgbd/osmosis_outdoor.pt\")\n    # print(f\"\\nArguments from inside main:\\n{args}\\n\")\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n    # print(args.unet_model)\n    # Prepare dataloader\n    data_config = args.data\n    # resize small side to be 256px, center cropping 256x256, normalizing to [-1,1]\n    transform = transforms.Compose([transforms.ToTensor(),\n                                    transforms.Resize(size=256),\n                                    transforms.CenterCrop(size=[256, 256]),\n                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n    # For the case of any data with ground truth (simulation in our case)\n    if data_config['ground_truth']:\n        gt_flag = True\n        dataset =ImagesFolder_GT(root_dir=data_config['root'], gt_rgb_dir=data_config['gt_rgb'],\n                                        gt_depth_dir=data_config['gt_depth'], transform=transform)\n        loader = DataLoader(dataset, batch_size=data_config['batch_size'], shuffle=False)\n\n    # for non ground truth dataset (underwater and haze for our case)\n    else:\n        gt_flag = False\n        data_config['root']=\"/kaggle/input/uw-image/data\"\n        dataset =ImagesFolder(data_config['root'], transform)\n        loader = DataLoader(dataset, batch_size=data_config['batch_size'], shuffle=False)\n\n    print(f\"\\nDataset size: {len(dataset)}\\n\")\n\n    #View content of Dataset like view image from dataloader\n    # for i in range(1):\n    #     sample = dataset[i]\n    #     print(sample)\n    #     image = sample[0]\n    #     print(image.shape)\n    #     image = tvtf.to_pil_image(image)\n    #     image.show()\n\n    model = create_model(**args.unet_model)\n    model = model.to(device)\n    model.eval()\n    measure_config = args.measurement\n    cond_config = args.conditioning\n    diffusion_config = args.diffusion\n    sample_pattern_config = args.sample_pattern\n    aux_loss_config = args.aux_loss\n\n\n    measurement_name = measure_config['operator']['name']\n    out_path = os.path.abspath(pjoin(args.save_dir, measurement_name, args.data['name']))\n    out_path = update_save_dir_date(out_path)\n\n    # create txt file with the configurations\n    yaml_to_txt(CONFIG_FILE, pjoin(out_path, f\"configurations.txt\"))\n\n\n    # directory for saving single results\n    if args.save_singles:\n        save_singles_path = pjoin(out_path, f\"single_images\")\n        os.makedirs(save_singles_path)\n\n        save_input_path = pjoin(save_singles_path, \"input\")\n        os.makedirs(save_input_path)\n        save_rgb_path = pjoin(save_singles_path, \"rgb\")\n        os.makedirs(save_rgb_path)\n        save_depth_pmm_color_path = pjoin(save_singles_path, \"depth_color\")\n        os.makedirs(save_depth_pmm_color_path)\n        save_depth_mm_path = pjoin(save_singles_path, \"depth_raw\")\n        os.makedirs(save_depth_mm_path)\n    else:\n        save_singles_path = None\n\n    # directory for the results a grid\n    if args.save_grids:\n        save_grids_path = pjoin(out_path, f\"grid_results\")\n        os.makedirs(save_grids_path)\n    else:\n        save_grids_path = None\n    \n    #Logging\n    logger.configure(dir=out_path)\n    logger.log(f\"pretrained model file: {args.unet_model['model_path']}\")\n    \n    if (not args.rgb_guidance):\n        log_txt_tmp = log_text(args=args)\n        logger.log(log_txt_tmp)\n\n    \n    for i, (ref_img, ref_img_name) in enumerate(loader):\n        # in case there is a GT image (if ground truth is used)\n        if gt_flag:\n            gt_rgb_img = ref_img[1].squeeze()\n            gt_rgb_img_01 = 0.5 * (gt_rgb_img + 1)\n\n            gt_depth_img = ref_img[2].squeeze()\n            gt_depth_img_01 = 0.5 * (gt_depth_img + 1)\n            gt_depth_img_01 = depth_tensor_to_color_image(gt_depth_img_01)\n\n            ref_img = ref_img[0]\n\n        start_run_time_ii = datetime.datetime.now()\n\n        # prepare reference image for visualization\n        ref_img_01 = 0.5 * (ref_img.detach().cpu()[0] + 1)\n        ref_img_name = ref_img_name[0]\n        orig_file_name = os.path.splitext(ref_img_name)[0]\n\n        # stop the run before getting to the last image\n        if i == args.data['stop_after']:\n            break\n        \n        # prepare operator for noise \n        measure_config['operator']['batch_size'] = args.data['batch_size']\n        operator = get_operator(device=device, **measure_config['operator'])\n        noiser = get_noise(**measure_config['noise'])\n\n\n        # Prepare conditioning - guidance method\n        cond_method = get_conditioning_method(cond_config['method'], operator, noiser, **cond_config['params'],\n                                              **sample_pattern_config, **aux_loss_config)\n        measurement_cond_fn = cond_method.conditioning\n\n        # Load diffusion sampler and pass the required arguments\n        sampler = create_sampler(**diffusion_config)\n        # passing the \"stable\" arguments with the partial method\n        sample_fn = partial(sampler.p_sample_loop, model=model, measurement_cond_fn=measurement_cond_fn,\n                            pretrain_model=args.unet_model['pretrain_model'], rgb_guidance=args.rgb_guidance,\n                            sample_pattern=args.sample_pattern,\n                            record=args.record_process,\n                            save_root=out_path, image_idx=i,\n                            record_every=args.record_every,\n                            original_file_name=orig_file_name,\n                            save_grids_path=save_grids_path)\n        \n        logger.log(f\"\\nInference image {i}: {ref_img_name}\\n\")\n        ref_img = ref_img.to(device)\n\n        # add noise to the image\n        y_n = noiser(ref_img)\n\n        # degamma the input image - use it for haze\n        if args.degamma_input:\n            y_n_tmp = 0.5 * (y_n + 1)\n            y_n = 2 * torch.pow(y_n_tmp, 2.2) - 1\n\n        # Sampling\n        x_start_shape = list(ref_img.shape)\n        # in case of sampling for osmosis the input model channel is 4 (RGBD)\n        x_start_shape[1] = 4 if (args.unet_model[\"pretrain_model\"] == 'osmosis') else x_start_shape[1]\n\n        # sampling noise for the begging of the diffusion model\n        if args.sample_pattern['pattern'] == \"original\":\n            global_N = 1\n        elif args.sample_pattern['pattern'] == \"pcgs\":\n            global_N = args.sample_pattern['global_N']\n        else:\n            raise ValueError(f\"Unrecognized sample pattern: {args.sample_pattern['pattern']}\")\n\n        # loop according the value of global N (from gibbsDDRM)\n        for global_ii in range(global_N):\n\n            logger.log(f\"global iteration: {global_ii}\\n\")\n            torch.manual_seed(args.manual_seed)\n\n            # the x_T - Gaussian Noise\n            x_start = torch.randn(x_start_shape, device=device).requires_grad_()\n\n            # this is the osmosis project additional code\n            if args.unet_model[\"pretrain_model\"] == 'osmosis' and not args.rgb_guidance:\n\n                # sampling function which adapted to osmosis project\n\n                sample, variable_dict, loss, out_xstart = sample_fn(x_start=x_start, measurement=y_n,\n                                                                    global_iteration=global_ii)\n\n                # output from the network without guidance - split into rgb and depth image\n                sample_rgb = out_xstart[0, 0:-1, :, :]\n                sample_depth_tmp = out_xstart[0, -1, :, :].unsqueeze(0)\n                sample_depth_tmp_rep = sample_depth_tmp.repeat(3, 1, 1)\n\n                # \"move\" the rgb predicted image to start from 0\n                sample_rgb_01 = 0.5 * (sample_rgb + 1)\n                sample_rgb_01_clip = torch.clamp(sample_rgb_01, min=0, max=1)\n\n                # \"move\" the depth predicted image to start from 0\n                sample_depth_mm = min_max_norm_range(sample_depth_tmp[0].unsqueeze(0))\n                sample_depth_vis_pmm = min_max_norm_range_percentile(sample_depth_tmp,\n                                                                            vmin=0, vmax=1,\n                                                                            percent_low=0.03,\n                                                                            percent_high=0.99,\n                                                                            is_uint8=False)\n                sample_depth_vis_pmm_color = depth_tensor_to_color_image(sample_depth_vis_pmm)\n\n                # depth for calculations\n                sample_depth_calc = convert_depth(sample_depth_tmp_rep,\n                                                         depth_type=args.measurement['operator']['depth_type'],\n                                                         value=args.measurement['operator']['value'])\n\n                # phi inf image - relevant for both underwater and haze\n                phi_inf = variable_dict['phi_inf'].cpu().squeeze(0)\n                phi_inf_image = phi_inf * torch.ones_like(sample_rgb, device=torch.device('cpu'))\n\n                # underwater model\n                if 'underwater_physical_revised' in args.measurement['operator']['name']:\n\n                    # create the ingredients for the underwater image\n                    phi_a = variable_dict['phi_a'].cpu().squeeze(0)\n                    phi_a_image = phi_a * torch.ones_like(sample_rgb, device=torch.device('cpu'))\n                    phi_b = variable_dict['phi_b'].cpu().squeeze(0)\n                    phi_b_image = phi_b * torch.ones_like(sample_rgb, device=torch.device('cpu'))\n\n                    # calculate the underwater parts\n                    backscatter_image = phi_inf_image * (1 - torch.exp(-phi_b_image * sample_depth_calc))\n                    attenuation_image = torch.exp(-phi_a_image * sample_depth_calc)\n                    forward_predicted_image = sample_rgb_01 * attenuation_image + backscatter_image\n\n                    # calculate norm lost for visualization - degraded_images and ref_img values should be [-1,1]\n                    degraded_image = 2 * forward_predicted_image - 1\n                    norm_loss_final = np.round([torch.linalg.norm(\n                        degraded_image - ref_img.detach().cpu()).numpy()], decimals=3)\n\n                    # calculate the \"clean\" image from the predicted phi's and ref image\n                    attenuation_flip_image = torch.exp(phi_a_image * sample_depth_calc)\n                    sample_rgb_recon = attenuation_flip_image * (ref_img_01 - backscatter_image)\n\n                    # logging values of phi's\n                    print_phi_a = [np.round(i, decimals=3) for i in phi_a.cpu().squeeze().tolist()]\n                    print_phi_b = [np.round(i, decimals=3) for i in phi_b.cpu().squeeze().tolist()]\n                    print_phi_inf = [np.round(i, decimals=3) for i in phi_inf.cpu().squeeze().tolist()]\n\n                    log_value_txt = f\"\\nInitialized values: \" \\\n                                    f\"\\nphi_a: [{measure_config['operator']['phi_a']}], lr: {measure_config['operator']['phi_a_eta']}\" \\\n                                    f\"\\nphi_b: [{measure_config['operator']['phi_b']}], lr: {measure_config['operator']['phi_b_eta']}\" \\\n                                    f\"\\nphi_inf: [{measure_config['operator']['phi_inf']}], lr: {measure_config['operator']['phi_inf_eta']}\" \\\n                                    f\"\\n\\nResults values: \" \\\n                                    f\"\\nphi_a: {print_phi_a}\" \\\n                                    f\"\\nphi_b: {print_phi_b}\" \\\n                                    f\"\\nphi_inf: {print_phi_inf}\" \\\n                                    f\"\\n\\nNorm loss: {norm_loss_final}\" \\\n                                    f\"\\nFinal loss: {np.round(np.array(loss), decimals=3)}\"\n\n                    # log results for parameters\n                    logger.log(log_value_txt)\n\n                # haze model\n                elif ('haze' in args.measurement['operator']['name']) or (\n                        'underwater_physical' in args.measurement['operator']['name']):\n\n                    # create the ingredients for the hazed image\n                    phi_ab = variable_dict['phi_ab'].cpu().squeeze(0)\n                    phi_ab_image = phi_ab * torch.ones_like(sample_rgb, device=torch.device('cpu'))\n                    backscatter_image = phi_inf_image * (1 - torch.exp(-phi_ab_image * sample_depth_calc))\n                    attenuation_image = torch.exp(-phi_ab_image * sample_depth_calc)\n                    forward_predicted_image = sample_rgb_01 * attenuation_image + backscatter_image\n\n                    # calculate the \"clean\" image from the predicted phis, phi_inf and ref image\n                    attenuation_flip_image = torch.exp(phi_ab_image * sample_depth_calc)\n                    sample_rgb_recon = attenuation_flip_image * (ref_img_01 - backscatter_image)\n\n                    # calculate norm lost for visualization - both degraded_images and ref_img values should be [-1,1]\n                    degraded_image = 2 * forward_predicted_image - 1\n                    norm_loss_final = np.round(\n                        [torch.linalg.norm(degraded_image.cpu() - ref_img.detach().cpu()).numpy()],\n                        decimals=3)\n\n                    # logging values of phi and phi_inf\n                    print_phi_ab = np.round(phi_ab.cpu().squeeze(), decimals=3)\n                    print_phi_inf = np.round(phi_inf.cpu().squeeze(), decimals=3)\n                    log_value_txt = f\"\\nInitialized values: \" \\\n                                    f\"\\nphi_ab: [{measure_config['operator']['phi_ab']}], lr: {measure_config['operator']['phi_ab_eta']}\" \\\n                                    f\"\\nphi_inf: [{measure_config['operator']['phi_inf']}], lr: {measure_config['operator']['phi_inf_eta']}\" \\\n                                    f\"\\n\\nResults values: \" \\\n                                    f\"\\nphi_ab: {print_phi_ab}\" \\\n                                    f\"\\nphi_inf: {print_phi_inf}\" \\\n                                    f\"\\n\\nNorm loss: {norm_loss_final}\" \\\n                                    f\"\\nFinal loss: {np.round(np.array(loss), decimals=5)}\"\n\n                    # log results for parameters\n                    logger.log(log_value_txt)\n\n                else:\n                    raise NotImplementedError(\"Operator can be for 'underwater' or 'haze' \")\n\n                # saving single images (reference (input), rgb (restored image), depth (depth estimation))\n                if args.save_singles:\n                    # input - reference image\n                    ref_im_pil = tvtf.to_pil_image(ref_img_01)\n                    # ref_im_pil.save(pjoin(save_singles_path, f'{orig_file_name}_g{global_ii}_ref.png'))\n                    ref_im_pil.save(pjoin(save_input_path, f'{orig_file_name}.png'))\n\n                    # rgb clip - sample_rgb_01_clip\n                    sample_rgb_01_clip_pil = tvtf.to_pil_image(sample_rgb_01_clip)\n                    # sample_rgb_01_clip_pil.save(pjoin(save_singles_path, f'{orig_file_name}_g{global_ii}_rgb.png'))\n                    sample_rgb_01_clip_pil.save(pjoin(save_rgb_path, f'{orig_file_name}.png'))\n\n                    # depth percentile min-max - sample_depth_vis_percentile_norm\n                    sample_depth_vis_pmm_color_pil = tvtf.to_pil_image(sample_depth_vis_pmm_color)\n                    # sample_depth_vis_pmm_color_pil.save(pjoin(save_singles_path, f'{orig_file_name}_g{global_ii}_depth.png'))\n                    sample_depth_vis_pmm_color_pil.save(pjoin(save_depth_pmm_color_path, f'{orig_file_name}.png'))\n\n                    # depth percentile min-max - sample_depth_vis_percentile_norm\n                    sample_depth_vis_mm_pil = tvtf.to_pil_image(sample_depth_mm)\n                    # sample_depth_vis_mm_pil.save(pjoin(save_singles_path, f'{orig_file_name}_g{global_ii}_depth_raw.png'))\n                    sample_depth_vis_mm_pil.save(pjoin(save_depth_mm_path, f'{orig_file_name}.png'))\n\n                # save extended results in the grid\n                if args.save_grids:\n\n                    grid_list = [ref_img_01, sample_rgb_01_clip, sample_depth_vis_pmm_color]\n\n                    # there is ground truth in the case of simulation\n                    if gt_flag:\n                        grid_list += [torch.zeros_like(sample_rgb_01, device=torch.device('cpu')),\n                                      gt_rgb_img_01, gt_depth_img_01]\n\n                    results_grid = make_grid(grid_list, nrow=3, pad_value=1.)\n                    results_grid = clip_image(results_grid, scale=False, move=False, is_uint8=True) \\\n                        .permute(1, 2, 0).numpy()\n                    results_pil = Image.fromarray(results_grid, mode=\"RGB\")\n\n                    # save the image\n                    results_pil.save(pjoin(save_grids_path, f'{orig_file_name}_g{global_ii}_grid.png'))\n\n                if args.save_singles or args.save_grids:\n                    logger.log(f\"result images was saved into: {out_path}\")\n\n                logger.log(f\"Run time: {datetime.datetime.now() - start_run_time_ii}\")\n\n            # no osmosis - rgb guidance\n            else:\n\n                sample = sample_fn(x_start=x_start, measurement=y_n)\n\n                # split into rgb and depth image - not handling results save for a batch of images\n                sample_rgb = sample.cpu()[0, 0:-1, :, :]\n                sample_depth_tmp = sample.cpu()[0, -1, :, :].repeat(3, 1, 1)\n\n                # \"move\" the rgb predicted image to start from 0 (the values \"sample_rgb\" should be between [-1, 1])\n                sample_rgb_01 = 0.5 * (sample_rgb + 1)\n                sample_rgb_01_clip = torch.clamp(sample_rgb_01, 0., 1.)\n\n                # used for visualization\n                sample_depth_mm = min_max_norm_range(sample_depth_tmp, vmin=0, vmax=1, is_uint8=False)\n                sample_depth_vis_pmm = min_max_norm_range_percentile(sample_depth_tmp,\n                                                                            percent_low=0.05, percent_high=0.99)\n                sample_depth_vis_pmm_color = depth_tensor_to_color_image(sample_depth_vis_pmm)\n\n                # saving seperated images\n                if args.save_singles:\n                    ref_im_pil = tvtf.to_pil_image(ref_img_01)\n                    ref_im_pil.save(pjoin(save_input_path, f'{orig_file_name}.png'))\n\n                    sample_rgb_pil = tvtf.to_pil_image(sample_rgb_01_clip)\n                    sample_rgb_pil.save(pjoin(save_rgb_path, f'{orig_file_name}.png'))\n\n                    sample_depth_vis_pil = tvtf.to_pil_image(sample_depth_vis_pmm_color)\n                    sample_depth_vis_pil.save(pjoin(save_depth_pmm_color_path, f'{orig_file_name}.png'))\n\n                    sample_depth_mm_pil = tvtf.to_pil_image(sample_depth_mm)\n                    sample_depth_mm_pil.save(pjoin(save_depth_mm_path, f'{orig_file_name}.png'))\n\n                # create images grid\n                if args.save_grids:\n                    grid_list = [ref_img_01, sample_rgb_01_clip, sample_depth_vis_pmm_color]\n                    results_grid = make_grid(grid_list, nrow=3, pad_value=1.)\n                    results_grid = clip_image(results_grid, scale=False, move=False, is_uint8=True)\n                    results_pil = tvtf.to_pil_image(results_grid)\n\n                    # save the image\n                    results_pil.save(pjoin(save_grids_path, f'{orig_file_name}.png'))\n\n                if args.save_singles or args.save_grids:\n                    logger.log(f\"result images was saved into: {out_path}\")\n\n                logger.log(f\"Run time: {datetime.datetime.now() - start_run_time_ii}\")\n\n    # close the logger txt file\n    logger.get_current().close()\n        \n\nif __name__ == \"__main__\":\n    # parser = ArgumentParser()\n    # parser.add_argument(\"-c\", \"--config_file\", default=\"/kaggle/input/yaml-file/osmosis_sample.yaml\", help=\"Configurations file\")\n    # parser.add_argument(\"-d\", \"--device\", default=0, help=\"GPU Device\", type=int)\n    # print(parser.parse_args())\n    args = {\"config_file\":\"/kaggle/input/yaml-file/osmosis_sample.yaml\",\"device\":0}\n    '''\n    vars is a function that converts an argument parser object into a dictionary of key-value pairs.\n    '''\n    # print(f\"\\nArguments from outside main:\\n{args}\\n\")\n\n    CONFIG_FILE = os.path.abspath(args[\"config_file\"])\n    '''\n    abspath is a function that returns the absolute path of a file or directory.\n    '''\n    DEVICE = args[\"device\"]\n\n    # print(f\"\\nConfiguration file:\\n{CONFIG_FILE}\\n\")\n\n    main()\n    print(f\"\\nFINISH!\")\n    sys.exit(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:59:17.722660Z","iopub.execute_input":"2024-11-13T09:59:17.723011Z","iopub.status.idle":"2024-11-13T10:17:55.245006Z","shell.execute_reply.started":"2024-11-13T09:59:17.722978Z","shell.execute_reply":"2024-11-13T10:17:55.242464Z"}},"outputs":[{"name":"stdout","text":"\nDataset size: 1\n\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2847094208.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(th.load(model_path, map_location='cpu'))\n","output_type":"stream"},{"name":"stdout","text":"Logging to /kaggle/working/results/underwater_physical_revised/osmosis/13-11-24/run5\npretrained model file: /kaggle/input/image-enhancer-rgbd/osmosis_outdoor.pt\n\n\nGuidance Scale: 7,7,7,0.9\nLoss Function: norm\nweight: depth, weight_function: gamma,1.4,1.4,1\nAuxiliary Loss: {'avrg_loss': 0.5, 'val_loss': 20}\nUnderwater model: underwater_physical_revised\nOptimize w.r.t: x_prev\nOptimizer model: sgd, \nManual seed: 0\nDepth type: gamma, value: 1.4,1.4,1\nNoise: clean\nGradient Clipping: True, min value: -0.005, max value: 0.005\nSample Pattern: pcgs, \n     Guidance start: 1 ,end: 0\n     Optimizations iters: 20, \n     Update start from: 0.7, end: 0\n     M: 1, start: 1, end: 0\n\nInference image 0: UW_Image.png\n\nglobal iteration: 0\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71fdaf7fbeeb4fb1bf3394998c01dd19"}},"metadata":{}},{"name":"stdout","text":"\nInitialized values: \nphi_a: [1.1,0.95,0.95], lr: 1e-5\nphi_b: [0.95, 0.8, 0.8], lr: 1e-5\nphi_inf: [0.14, 0.29, 0.49], lr: 1e-5\n\nResults values: \nphi_a: [1.109, 0.968, 0.738]\nphi_b: [0.946, 0.774, 0.693]\nphi_inf: [0.066, 0.202, 0.366]\n\nNorm loss: [7.019]\nFinal loss: [3.113]\nresult images was saved into: /kaggle/working/results/underwater_physical_revised/osmosis/13-11-24/run5\nRun time: 0:18:30.570036\n\nFINISH!\n","output_type":"stream"},{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"],"ename":"SystemExit","evalue":"0","output_type":"error"}],"execution_count":46},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/logger')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:56:49.159042Z","iopub.execute_input":"2024-11-13T09:56:49.159507Z","iopub.status.idle":"2024-11-13T09:56:49.164510Z","shell.execute_reply.started":"2024-11-13T09:56:49.159464Z","shell.execute_reply":"2024-11-13T09:56:49.163395Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import logger ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:57:01.317971Z","iopub.execute_input":"2024-11-13T09:57:01.318421Z","iopub.status.idle":"2024-11-13T09:57:01.333014Z","shell.execute_reply.started":"2024-11-13T09:57:01.318379Z","shell.execute_reply":"2024-11-13T09:57:01.332133Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"\"\"\"\nVarious utilities for neural networks.\n\"\"\"\n\nimport math\n\nimport torch as th\nimport torch.nn as nn\n\n\n# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\nclass SiLU(nn.Module):\n    def forward(self, x):\n        return x * th.sigmoid(x)\n\n\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\n\n\ndef conv_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D convolution module.\n    \"\"\"\n    if dims == 1:\n        return nn.Conv1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.Conv2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.Conv3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef linear(*args, **kwargs):\n    \"\"\"\n    Create a linear module.\n    \"\"\"\n    return nn.Linear(*args, **kwargs)\n\n\ndef avg_pool_nd(dims, *args, **kwargs):\n    \"\"\"\n    Create a 1D, 2D, or 3D average pooling module.\n    \"\"\"\n    if dims == 1:\n        return nn.AvgPool1d(*args, **kwargs)\n    elif dims == 2:\n        return nn.AvgPool2d(*args, **kwargs)\n    elif dims == 3:\n        return nn.AvgPool3d(*args, **kwargs)\n    raise ValueError(f\"unsupported dimensions: {dims}\")\n\n\ndef update_ema(target_params, source_params, rate=0.99):\n    \"\"\"\n    Update target parameters to be closer to those of source parameters using\n    an exponential moving average.\n\n    :param target_params: the target parameter sequence.\n    :param source_params: the source parameter sequence.\n    :param rate: the EMA rate (closer to 1 means slower).\n    \"\"\"\n    for targ, src in zip(target_params, source_params):\n        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\n\n\ndef scale_module(module, scale):\n    \"\"\"\n    Scale the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().mul_(scale)\n    return module\n\n\ndef mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n\n\ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    return GroupNorm32(32, channels)\n\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = th.exp(\n        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n    ).to(device=timesteps.device)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding\n\n\ndef checkpoint(func, inputs, params, flag):\n    \"\"\"\n    Evaluate a function without caching intermediate activations, allowing for\n    reduced memory at the expense of extra compute in the backward pass.\n\n    :param func: the function to evaluate.\n    :param inputs: the argument sequence to pass to `func`.\n    :param params: a sequence of parameters `func` depends on but does not\n                   explicitly take as arguments.\n    :param flag: if False, disable gradient checkpointing.\n    \"\"\"\n    if flag:\n        args = tuple(inputs) + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)\n\n\nclass CheckpointFunction(th.autograd.Function):\n    @staticmethod\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n        with th.no_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        return output_tensors\n\n    @staticmethod\n    def backward(ctx, *output_grads):\n        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n        with th.enable_grad():\n            # Fixes a bug where the first op in run_function modifies the\n            # Tensor storage in place, which is not allowed for detach()'d\n            # Tensors.\n            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n            output_tensors = ctx.run_function(*shallow_copies)\n        input_grads = th.autograd.grad(\n            output_tensors,\n            ctx.input_tensors + ctx.input_params,\n            output_grads,\n            allow_unused=True,\n        )\n        del ctx.input_tensors\n        del ctx.input_params\n        del output_tensors\n        return (None, None) + input_grads","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T09:59:02.525685Z","iopub.execute_input":"2024-11-13T09:59:02.526068Z","iopub.status.idle":"2024-11-13T09:59:02.553444Z","shell.execute_reply.started":"2024-11-13T09:59:02.526031Z","shell.execute_reply":"2024-11-13T09:59:02.552291Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}